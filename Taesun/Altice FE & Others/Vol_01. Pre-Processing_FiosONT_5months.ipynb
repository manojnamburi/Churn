{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Process TRAIN/TEST Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Custom Transformers for Pre-Processing\n",
    "**Custom Transformers** will conduct the following pre-processing:\n",
    "- Missing values and imputation\n",
    "    - `Remove_MissingFeatures`\n",
    "        - Identify missing percentages of features.\n",
    "        - Remove features with missing % >= threshold missing %.\n",
    "        - **Note**: Be applied prior to **any missing value imputation**.\n",
    "- Zero/near-zero variance features\n",
    "    - `Remove_ConstantFeatures`\n",
    "        - Identify features with a single unique value.\n",
    "        - Remove those constant features.\n",
    "- Duplicate/highly correlated features\n",
    "    - `Remove_CorrelatedFeatures`\n",
    "        - Compute pairwise correlation between features.\n",
    "        - Remove features with abs(correlation) >= threshold correlation.\n",
    "        - **Note**: Relevant to **numerical features only**.\n",
    "    - `Remove_DuplicateFeatures`\n",
    "        - Identify features with duplicate columns.\n",
    "        - Remove features with duplicate columns.\n",
    "- Data Type Conversion\n",
    "    - `Use_DefaultDataType`\n",
    "        - Identify features having data types inconsistent with default data types.\n",
    "        - Convert the data types into the default data types if inconsistent.\n",
    "        - **Note**: No feature removed!\n",
    "- Default Imputation\n",
    "    - `Use_DefaultImputer`\n",
    "        - Use default imputation values.\n",
    "        - **Note**: No feature removed!            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. TRAIN/TEST Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 0. Import Required Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "### Import Customer Transformers\n",
    "import PreProcessing_Custom_Transformers_v2 as PP\n",
    "import FeatureEngineering_Custom_Transformers as FE\n",
    "import FeatureCreation_Custom_Transformers as FC\n",
    "\n",
    "# Use the Updated Attribute/Imputation Dictionaries!!!\n",
    "%run 'data_new/attribute_dictionary.py'\n",
    "%run 'data_new/imputation_dictionary.py'\n",
    "\n",
    "# Remove DataConversionWarning\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "TRAIN vs TEST Datasets\n",
      "**************************************************\n",
      "Competitive Area:  ['Fios ONT Competitive Area']\n",
      "The Shape of TRAIN Data: (102451, 1023)\n",
      "The Shape of TEST Data:  (100858, 1023)\n",
      "\n",
      "**************************************************\n",
      "Overall Churn Rate\n",
      "**************************************************\n",
      "TRAIN:  0.0367\n",
      "TEST:   0.0405 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Competitive Area = 'Fios ONT'\n",
    "df_train = pd.read_pickle('data_new/Vol_df_train_Fios ONT Competitive Area_5months.pkl')\n",
    "df_test  = pd.read_pickle('data_new/Vol_df_test_Fios ONT Competitive Area_5months.pkl')\n",
    "\n",
    "# Use 'products' to choose eligible customers.\n",
    "df_train = df_train[(df_train['products'].isin(['2: Video/OOL','3: Video/OOL/OV']))]\n",
    "df_test  = df_test[(df_test['products'].isin(['2: Video/OOL','3: Video/OOL/OV']))]\n",
    "\n",
    "# Use 'chc_id' as index, and sort by index.\n",
    "df_train.set_index('chc_id', inplace=True)\n",
    "df_test.set_index('chc_id', inplace=True)\n",
    "\n",
    "df_train = df_train.sort_index()\n",
    "df_test  = df_test.sort_index()\n",
    "\n",
    "# TRAIN\n",
    "train_X = df_train.drop('status', axis=1).copy()\n",
    "train_y = df_train['status']\n",
    "\n",
    "# TEST\n",
    "test_X  = df_test.drop('status', axis=1).copy()\n",
    "test_y  = df_test['status']\n",
    "\n",
    "# Sample Size\n",
    "print('*'*50 + '\\nTRAIN vs TEST Datasets\\n' + '*'*50)\n",
    "print('Competitive Area: ', df_train.competitive_area.unique())\n",
    "print('The Shape of TRAIN Data: ' + str(df_train.shape))\n",
    "print('The Shape of TEST Data:  ' + str(df_test.shape))\n",
    "\n",
    "## Churn Rate by Sample Type\n",
    "print('\\n' + '*'*50 + '\\nOverall Churn Rate\\n' + '*'*50)\n",
    "print('TRAIN: ', df_train.status.value_counts(normalize=True)[1].round(4))\n",
    "print('TEST:  ', df_test.status.value_counts(normalize=True)[1].round(4), '\\n')\n",
    "\n",
    "# print(train_X.index)\n",
    "# print(train_y.index)\n",
    "# print(test_X.index)\n",
    "# print(test_y.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Pre-Processing: Use_DefaultDataType\n",
      "**************************************************\n",
      "- It will convert data types into default ones.\n",
      "\n",
      "**************************************************\n",
      "Pre-Processing: Remove_MissingFeatures\n",
      "**************************************************\n",
      "- It will remove features with a high missing pct.\n",
      "\n",
      "**************************************************\n",
      "Pre-Processing: Remove_ConstantFeatures\n",
      "**************************************************\n",
      "- It will remove features with 1 unique value(s).\n",
      "\n",
      "**************************************************\n",
      "Pre-Processing: Remove_CorrelatedFeatures\n",
      "**************************************************\n",
      "- It will work on Numerical Features Only, doing nothing on Categorical Features.\n",
      "- It may take 10+ minutes. Be patient!\n",
      "\n",
      "**************************************************\n",
      "Pre-Processing: Remove_DuplicateFeatures\n",
      "**************************************************\n",
      "- It may take 10+ minutes. Be patient!\n",
      "\n",
      "**************************************************\n",
      "Pre-Processing: Use_DefaultImputere\n",
      "**************************************************\n",
      "- It will append default imputation values to missings.\n",
      "\n",
      "**************************************************\n",
      "Pre-Processing: Remove_ConstantFeatures\n",
      "**************************************************\n",
      "- It will remove features with 1 unique value(s).\n",
      "\n",
      "**************************************************\n",
      "Pre-Processing: Remove_CorrelatedFeatures\n",
      "**************************************************\n",
      "- It will work on Numerical Features Only, doing nothing on Categorical Features.\n",
      "- It may take 10+ minutes. Be patient!\n",
      "\n",
      "29 features with greater than 99.0% missing values\n",
      "27 features with 1 or fewer unique value(s)\n",
      "46 features with abs(correlation ) > 0.99 with other features\n",
      "3 features with duplicate columns\n",
      "14 features with 1 or fewer unique value(s)\n",
      "128 features with abs(correlation ) > 0.9 with other features\n",
      "\n",
      "**************************************************\n",
      "Before vs After Transformation\n",
      "**************************************************\n",
      "TRAIN: Before Transformation:(102451, 1022)\n",
      "TRAIN: After Transformation: (102451, 760)\n",
      "TEST:  After Transformation: (100858, 760)\n",
      "CPU times: user 45min 43s, sys: 9min 32s, total: 55min 15s\n",
      "Wall time: 53min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# (1) Make a Pipeline and Instantiate\n",
    "Pipe_PP = Pipeline([\n",
    "                    ('DataType', PP.Use_DefaultDataType(default_dtypes=attribute_dict)),\n",
    "                    ('Missing', PP.Remove_MissingFeatures(missing_threshold=0.99)), \n",
    "                    ('Constant1', PP.Remove_ConstantFeatures(unique_threshold=1, missing_threshold=0.00)), \n",
    "                    ('Correlated1', PP.Remove_CorrelatedFeatures(correlation_threshold=0.99)), \n",
    "                    ('Duplicate', PP.Remove_DuplicateFeatures()),\n",
    "                    ('Imputer', PP.Use_DefaultImputer(default_imputers=attribute_imputer_dict, default_dtypes=attribute_dict)),\n",
    "                    ('Constant2', PP.Remove_ConstantFeatures(unique_threshold=1, missing_threshold=0.00)), \n",
    "                    ('Correlated2', PP.Remove_CorrelatedFeatures(correlation_threshold=0.90))\n",
    "                  ])\n",
    "\n",
    "# 'Constant2' is added to handle (1) unique value = 0 and (2) default imputation value = 0.\n",
    "# 'Correlated2' is added to further remove correlated features after impuation.\n",
    "\n",
    "\n",
    "# (2) fit()\n",
    "Pipe_PP.fit(train_X, train_y)\n",
    "\n",
    "\n",
    "# (3) transform()\n",
    "train_X_PP = Pipe_PP.transform(train_X)\n",
    "test_X_PP  = Pipe_PP.transform(test_X)\n",
    "\n",
    "# Feature Dimension\n",
    "print('\\n' + '*'*50 + '\\nBefore vs After Transformation\\n' + '*'*50)\n",
    "print('TRAIN: Before Transformation:' + str(train_X.shape))\n",
    "print('TRAIN: After Transformation: ' + str(train_X_PP.shape))\n",
    "print('TEST:  After Transformation: ' + str(test_X_PP.shape))\n",
    "\n",
    "# CPU times: user 46min 8s, sys: 8min 53s, total: 55min 2s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Pre-Processing: Use_DefaultImputere\n",
      "**************************************************\n",
      "- It will append default imputation values to missings.\n",
      "\n",
      "\n",
      "**************************************************\n",
      "Before vs After Transformation\n",
      "**************************************************\n",
      "TRAIN: Before Transformation:(102451, 1022)\n",
      "TRAIN: After Transformation: (102451, 16)\n",
      "TEST:  After Transformation: (100858, 16)\n",
      "\n",
      "**************************************************\n",
      "Newly Created Features\n",
      "**************************************************\n",
      " ['grp_tenure_3m', 'grp_tenure_1m', 'grp_tenure_6m', 'grp_payment_method', 'grp_payment_25dollar', 'grp_payment_10dollar', 'grp_payment_change_5dollar', 'grp_payment_change_10dollar', 'grp_payment_change_2pct', 'grp_payment_change_5pct', 'ratio_payment_income', 'grp_payment_income', 'grp_call_csc', 'grp_call_bill', 'grp_call_csr', 'grp_call_tsr']\n",
      "CPU times: user 16min 36s, sys: 4min 59s, total: 21min 36s\n",
      "Wall time: 20min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# (1) Make a Pipeline and Instantiate\n",
    "Pipe_NF = Pipeline([\n",
    "                    ('Imputer', PP.Use_DefaultImputer(default_imputers=attribute_imputer_dict, default_dtypes=attribute_dict)),\n",
    "                    ('NewFeatures', FC.FeatureMaker())\n",
    "                  ])\n",
    "\n",
    "# 'Imputer' is added to handle missing values\n",
    "# 'NewFeature' is added to create new features\n",
    "\n",
    "\n",
    "# (2) fit()\n",
    "Pipe_NF.fit(train_X, train_y)\n",
    "\n",
    "\n",
    "# (3) transform()\n",
    "train_X_NF = Pipe_NF.transform(train_X)\n",
    "test_X_NF  = Pipe_NF.transform(test_X)\n",
    "\n",
    "# Feature Dimension\n",
    "print('\\n' + '*'*50 + '\\nBefore vs After Transformation\\n' + '*'*50)\n",
    "print('TRAIN: Before Transformation:' + str(train_X.shape))\n",
    "print('TRAIN: After Transformation: ' + str(train_X_NF.shape))\n",
    "print('TEST:  After Transformation: ' + str(test_X_NF.shape))\n",
    "print('\\n' + '*'*50 + '\\nNewly Created Features\\n' + '*'*50 + '\\n', \n",
    "      Pipe_NF.named_steps['NewFeatures'].features_new_)\n",
    "\n",
    "# CPU times: user 17min 27s, sys: 4min 27s, total: 21min 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_X_NF.groupby('grp_tenure_3m').count()\n",
    "# train_X_NF.grp_tenure_3m.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Processed and New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102451, 777)\n",
      "(100858, 777)\n"
     ]
    }
   ],
   "source": [
    "# Create Datasets that Consist of Pre-processed and New Features.\n",
    "df_train_NF_PP = train_y.to_frame().\\\n",
    "                 merge(train_X_NF, how='inner', left_index=True, right_index=True).\\\n",
    "                 merge(train_X_PP, how='inner', left_index=True, right_index=True)\n",
    "\n",
    "df_test_NF_PP  = test_y.to_frame().\\\n",
    "                 merge(test_X_NF, how='inner', left_index=True, right_index=True).\\\n",
    "                 merge(test_X_PP, how='inner', left_index=True, right_index=True)\n",
    "\n",
    "# Save Data for Feature Engineering\n",
    "# Pre-processed data with new features\n",
    "df_train_NF_PP.to_pickle('data_new/Vol_df_train_FiosONT_PP_5months.pkl')\n",
    "df_test_NF_PP.to_pickle('data_new/Vol_df_test_FiosONT_PP_5months.pkl')\n",
    "\n",
    "print(df_train_NF_PP.shape)\n",
    "print(df_test_NF_PP.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
