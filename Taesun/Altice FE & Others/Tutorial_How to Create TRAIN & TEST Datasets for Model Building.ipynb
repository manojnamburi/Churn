{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create TRAIN/TEST Datasets for Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Load Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Default Packages\n",
    "import pandas as pd\n",
    "import pandas_gbq as gbq\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "# import os\n",
    "import string\n",
    "# import matplotlib.cm as cm\n",
    "# from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "### Pipeline Packages\n",
    "from sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "### Custom Transformers\n",
    "import Altice_FeatureEngineering as FE                         # Include 'competitive_area' as a feature\n",
    "from Class_GeneralUtilitiesNonTF import GeneralUtilitiesNonTF  # For BQ-Storage-Python Connection\n",
    "\n",
    "\n",
    "### Dictionaries for Data Type and Missing Value Imputation\n",
    "from attribute_dictionary import attribute_dict\n",
    "from imputation_dictionary import attribute_imputer_dict\n",
    "\n",
    "\n",
    "### Remove DataConversionWarning\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import TRAIN/TEST Datasets from BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing TRAIN/TEST Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do Not Change the Below Information Unless Use Others!\n",
    "bucket     = 'alticeusa-am'\n",
    "project_id = 'alticeusa-am'\n",
    "dataset    = 'poc'\n",
    "auth_file  = 'alticeusa-am-b639e404289b.json'\n",
    "\n",
    "#Instantiate the util obj that will be used to interact with BQ\n",
    "util_obj   = GeneralUtilitiesNonTF(project_id  = project_id, \n",
    "                                   dataset     = dataset,\n",
    "                                   bucket_name = bucket,\n",
    "                                   json_path   = auth_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temp table to be created: temp_gcs_06_05_19_03_36_16\n",
      "**********\n",
      "Query executed: \n",
      " SELECT * FROM poc.YST_OPTION1_TRAIN WHERE Segment='FIOS_ONT_G1_4'; \n",
      "Temp table temp_gcs_06_05_19_03_36_16 created\n",
      "**********\n",
      "Info query: SELECT table_name,column_name,data_type,is_nullable FROM poc.INFORMATION_SCHEMA.COLUMNS WHERE table_name='temp_gcs_06_05_19_03_36_16'\n",
      "Extraction to the tmp_folder_06_05_19_03_36_16 complete\n",
      "Prefix: tmp_folder_06_05_19_03_36_16/tmp_file_06_05_19_03_36_16\n",
      "Pulling file: tmp_folder_06_05_19_03_36_16/tmp_file_06_05_19_03_36_16-000000000000.csv.gzip\n",
      "Pulling file: tmp_folder_06_05_19_03_36_16/tmp_file_06_05_19_03_36_16-000000000001.csv.gzip\n",
      "Deleted the temp folder tmp_folder_06_05_19_03_36_16\n",
      "\n",
      "*****************************************************************\n",
      " TRAIN data is SUCCESSFULLY imported! Number of records = 106176.\n",
      "*****************************************************************\n",
      "\n",
      "Temp table to be created: temp_gcs_06_05_19_03_37_50\n",
      "**********\n",
      "Query executed: \n",
      " SELECT * FROM poc.YST_OPTION1_TEST WHERE Segment='FIOS_ONT_G1_4'; \n",
      "Temp table temp_gcs_06_05_19_03_37_50 created\n",
      "**********\n",
      "Info query: SELECT table_name,column_name,data_type,is_nullable FROM poc.INFORMATION_SCHEMA.COLUMNS WHERE table_name='temp_gcs_06_05_19_03_37_50'\n",
      "Extraction to the tmp_folder_06_05_19_03_37_50 complete\n",
      "Prefix: tmp_folder_06_05_19_03_37_50/tmp_file_06_05_19_03_37_50\n",
      "Pulling file: tmp_folder_06_05_19_03_37_50/tmp_file_06_05_19_03_37_50-000000000000.csv.gzip\n",
      "Pulling file: tmp_folder_06_05_19_03_37_50/tmp_file_06_05_19_03_37_50-000000000001.csv.gzip\n",
      "Deleted the temp folder tmp_folder_06_05_19_03_37_50\n",
      "\n",
      "*****************************************************************\n",
      " TEST data is SUCCESSFULLY imported! Number of records = 102062.\n",
      "*****************************************************************\n",
      "\n",
      "CPU times: user 35.6 s, sys: 7.54 s, total: 43.2 s\n",
      "Wall time: 4min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# TRAIN Dataset\n",
    "sql_data = ''' SELECT * FROM poc.YST_OPTION1_TRAIN WHERE Segment='FIOS_ONT_G1_4'; ''' \n",
    "df_train = util_obj.read_gbq(sql_query = sql_data)\n",
    "print('\\n' + '*'*65 + '\\n', f\"TRAIN data is SUCCESSFULLY imported! Number of records = {str(len(df_train))}.\" \\\n",
    "      + '\\n' + '*'*65 + '\\n')\n",
    "               \n",
    "# TEST Dataset\n",
    "sql_data = ''' SELECT * FROM poc.YST_OPTION1_TEST WHERE Segment='FIOS_ONT_G1_4'; ''' \n",
    "df_test  = util_obj.read_gbq(sql_query = sql_data)\n",
    "print('\\n' + '*'*65 + '\\n', f\"TEST data is SUCCESSFULLY imported! Number of records = {str(len(df_test))}.\" \\\n",
    "      + '\\n' + '*'*65 + '\\n')\n",
    "\n",
    "# For Other Segments\n",
    "# FIOS_ONT_G1_4\n",
    "# FIOS_ONT_G4_8\n",
    "# FIOS_COMP_G1_4\n",
    "# Other_Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "TRAIN vs TEST Datasets\n",
      "**************************************************\n",
      "Competitive Area:  ['Fios ONT Competitive Area']\n",
      "The Shape of TRAIN Data: (106176, 1037)\n",
      "The Shape of TEST Data:  (102062, 1037)\n",
      "\n",
      "**************************************************\n",
      "Overall Churn Rate\n",
      "**************************************************\n",
      "TRAIN:  0.025\n",
      "TEST:   0.033 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use 'chc_id' as index, and sort by index.\n",
    "df_train.set_index('chc_id', inplace=True)\n",
    "df_test.set_index('chc_id', inplace=True)\n",
    "\n",
    "df_train = df_train.sort_index()\n",
    "df_test  = df_test.sort_index()\n",
    "\n",
    "# Drop 'Segment'.\n",
    "df_train.drop('Segment', axis=1, inplace=True)\n",
    "df_test.drop('Segment', axis=1, inplace=True)\n",
    "\n",
    "# TRAIN\n",
    "train_X  = df_train.drop('status', axis=1).copy()\n",
    "train_y  = df_train['status']\n",
    "\n",
    "# TEST\n",
    "test_X   = df_test.drop('status', axis=1).copy()\n",
    "test_y   = df_test['status']\n",
    "\n",
    "# Sample Size\n",
    "print('*'*50 + '\\nTRAIN vs TEST Datasets\\n' + '*'*50)\n",
    "print('Competitive Area: ', df_train.competitive_area.unique())\n",
    "print('The Shape of TRAIN Data: ' + str(df_train.shape))\n",
    "print('The Shape of TEST Data:  ' + str(df_test.shape))\n",
    "\n",
    "## Churn Rate by Sample Type\n",
    "print('\\n' + '*'*50 + '\\nOverall Churn Rate\\n' + '*'*50)\n",
    "print('TRAIN: ', df_train.status.value_counts(normalize=True)[1].round(3))\n",
    "print('TEST:  ', df_test.status.value_counts(normalize=True)[1].round(3), '\\n')\n",
    "\n",
    "# print(train_X.index)\n",
    "# print(train_y.index)\n",
    "# print(test_X.index)\n",
    "# print(test_y.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-Process TRAIN/TEST Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Pre-Processing: Use_DefaultDataType\n",
      "**************************************************\n",
      "- It will convert data types into default ones.\n",
      "\n",
      "**************************************************\n",
      "Pre-Processing: Remove_MissingFeatures\n",
      "**************************************************\n",
      "- It will remove features with a high missing pct.\n",
      "\n",
      "**************************************************\n",
      "Pre-Processing: Remove_ConstantFeatures\n",
      "**************************************************\n",
      "- It will remove features with 1 unique value(s).\n",
      "\n",
      "**************************************************\n",
      "Pre-Processing: Remove_CorrelatedFeatures\n",
      "**************************************************\n",
      "- It will work on Numerical Features Only, doing nothing on Categorical Features.\n",
      "- It may take 10+ minutes. Be patient!\n",
      "\n",
      "**************************************************\n",
      "Pre-Processing: Remove_DuplicateFeatures\n",
      "**************************************************\n",
      "- It may take 10+ minutes. Be patient!\n",
      "\n",
      "**************************************************\n",
      "Pre-Processing: Use_DefaultImputere\n",
      "**************************************************\n",
      "- It will append default imputation values to missings.\n",
      "\n",
      "**************************************************\n",
      "Pre-Processing: Remove_ConstantFeatures\n",
      "**************************************************\n",
      "- It will remove features with 1 unique value(s).\n",
      "\n",
      "**************************************************\n",
      "Pre-Processing: Remove_CorrelatedFeatures\n",
      "**************************************************\n",
      "- It will work on Numerical Features Only, doing nothing on Categorical Features.\n",
      "- It may take 10+ minutes. Be patient!\n",
      "\n",
      "30 features with greater than 99.0% missing values\n",
      "28 features with 1 or fewer unique value(s)\n",
      "46 features with abs(correlation ) > 0.99 with other features\n",
      "2 features with duplicate columns\n",
      "14 features with 1 or fewer unique value(s)\n",
      "130 features with abs(correlation ) > 0.9 with other features\n",
      "\n",
      "**************************************************\n",
      "Before vs After Transformation\n",
      "**************************************************\n",
      "TRAIN: Before Transformation:(106176, 1036)\n",
      "TRAIN: After Transformation: (106176, 773)\n",
      "TEST:  After Transformation: (102062, 773)\n",
      "CPU times: user 27min, sys: 9min 15s, total: 36min 16s\n",
      "Wall time: 36min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# (1) Make a Pipeline and Instantiate\n",
    "Pipe_PP = Pipeline([\n",
    "                    ('DataType', FE.Use_DefaultDataType(default_dtypes=attribute_dict)),\n",
    "                    ('Missing', FE.Remove_MissingFeatures(missing_threshold=0.99)), \n",
    "                    ('Constant1', FE.Remove_ConstantFeatures(unique_threshold=1, missing_threshold=0.00)), \n",
    "                    ('Correlated1', FE.Remove_CorrelatedFeatures(correlation_threshold=0.99)), \n",
    "                    ('Duplicate', FE.Remove_DuplicateFeatures()),\n",
    "                    ('Imputer', FE.Use_DefaultImputer(default_imputers=attribute_imputer_dict, default_dtypes=attribute_dict)),\n",
    "                    ('Constant2', FE.Remove_ConstantFeatures(unique_threshold=1, missing_threshold=0.00)), \n",
    "                    ('Correlated2', FE.Remove_CorrelatedFeatures(correlation_threshold=0.90))\n",
    "                  ])\n",
    "\n",
    "# 'Constant2' is added to handle (1) unique value = 0 and (2) default imputation value = 0.\n",
    "# 'Correlated2' is added to further remove correlated features after impuation.\n",
    "\n",
    "\n",
    "# (2) fit()\n",
    "Pipe_PP.fit(train_X, train_y)\n",
    "\n",
    "\n",
    "# (3) transform()\n",
    "train_X_PP = Pipe_PP.transform(train_X)\n",
    "test_X_PP  = Pipe_PP.transform(test_X)\n",
    "\n",
    "# Feature Dimension\n",
    "print('\\n' + '*'*50 + '\\nBefore vs After Transformation\\n' + '*'*50)\n",
    "print('TRAIN: Before Transformation:' + str(train_X.shape))\n",
    "print('TRAIN: After Transformation: ' + str(train_X_PP.shape))\n",
    "print('TEST:  After Transformation: ' + str(test_X_PP.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Pre-Processing: Use_DefaultImputere\n",
      "**************************************************\n",
      "- It will append default imputation values to missings.\n",
      "\n",
      "\n",
      "**************************************************\n",
      "Before vs After Transformation\n",
      "**************************************************\n",
      "TRAIN: Before Transformation:(106176, 1036)\n",
      "TRAIN: After Transformation: (106176, 16)\n",
      "TEST:  After Transformation: (102062, 16)\n",
      "\n",
      "**************************************************\n",
      "Newly Created Features\n",
      "**************************************************\n",
      " ['grp_tenure_3m', 'grp_tenure_1m', 'grp_tenure_6m', 'grp_payment_method', 'grp_payment_25dollar', 'grp_payment_10dollar', 'grp_payment_change_5dollar', 'grp_payment_change_10dollar', 'grp_payment_change_2pct', 'grp_payment_change_5pct', 'ratio_payment_income', 'grp_payment_income', 'grp_call_csc', 'grp_call_bill', 'grp_call_csr', 'grp_call_tsr']\n",
      "CPU times: user 9min 7s, sys: 5min 46s, total: 14min 54s\n",
      "Wall time: 14min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# (1) Make a Pipeline and Instantiate\n",
    "Pipe_NF = Pipeline([\n",
    "                    ('Imputer', FE.Use_DefaultImputer(default_imputers=attribute_imputer_dict, default_dtypes=attribute_dict)),\n",
    "                    ('NewFeatures', FE.FeatureMaker())\n",
    "                  ])\n",
    "\n",
    "# 'Imputer' is added to handle missing values\n",
    "# 'NewFeature' is added to create new features\n",
    "\n",
    "\n",
    "# (2) fit()\n",
    "Pipe_NF.fit(train_X, train_y)\n",
    "\n",
    "\n",
    "# (3) transform()\n",
    "train_X_NF = Pipe_NF.transform(train_X)\n",
    "test_X_NF  = Pipe_NF.transform(test_X)\n",
    "\n",
    "# Feature Dimension\n",
    "print('\\n' + '*'*50 + '\\nBefore vs After Transformation\\n' + '*'*50)\n",
    "print('TRAIN: Before Transformation:' + str(train_X.shape))\n",
    "print('TRAIN: After Transformation: ' + str(train_X_NF.shape))\n",
    "print('TEST:  After Transformation: ' + str(test_X_NF.shape))\n",
    "print('\\n' + '*'*50 + '\\nNewly Created Features\\n' + '*'*50 + '\\n', \n",
    "      Pipe_NF.named_steps['NewFeatures'].features_new_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Processed and New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106176, 790)\n",
      "(102062, 790)\n",
      "CPU times: user 900 ms, sys: 672 ms, total: 1.57 s\n",
      "Wall time: 1.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create Datasets that Consist of Pre-processed and New Features.\n",
    "df_train_NF_PP = pd.concat([train_y.to_frame(), train_X_NF, train_X_PP], axis=1)\n",
    "df_test_NF_PP  = pd.concat([test_y.to_frame(), test_X_NF, test_X_PP], axis=1)\n",
    "\n",
    "# Save Data for Feature Engineering\n",
    "# Pre-processed data with new features\n",
    "# df_train_NF_PP.to_pickle('data_new/FIOS_ONT_G1_4_train_PP.pkl')\n",
    "# df_test_NF_PP.to_pickle('data_new/FIOS_ONT_G1_4_test_PP.pkl')\n",
    "\n",
    "print(df_train_NF_PP.shape)\n",
    "print(df_test_NF_PP.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106176, 789)\n",
      "(102062, 789)\n"
     ]
    }
   ],
   "source": [
    "# Use Pre-Processed Data as new TRAIN/TEST Datasets\n",
    "train_X = pd.concat([train_X_NF, train_X_PP], axis=1)\n",
    "test_X  = pd.concat([test_X_NF, test_X_PP], axis=1)\n",
    "print(train_X.shape)\n",
    "print(test_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-Use Pre-Processed Data! Pre-Processing Takes an 1 Hour!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = pd.read_pickle('data_new/FIOS_ONT_G1_4_train_PP.pkl')\n",
    "# df_test  = pd.read_pickle('data_new/FIOS_ONT_G1_4_test_PP.pkl')\n",
    "\n",
    "# # TRAIN\n",
    "# train_X  = df_train.drop('status', axis=1).copy()\n",
    "# train_y  = df_train['status']\n",
    "\n",
    "# # TEST\n",
    "# test_X   = df_test.drop('status', axis=1).copy()\n",
    "# test_y   = df_test['status']\n",
    "\n",
    "# # Sample Size\n",
    "# print('*'*50 + '\\nTRAIN vs TEST Datasets\\n' + '*'*50)\n",
    "# print('The Shape of TRAIN Data: ' + str(df_train.shape))\n",
    "# print('The Shape of TEST Data:  ' + str(df_test.shape))\n",
    "\n",
    "# ## Churn Rate by Sample Type\n",
    "# print('\\n' + '*'*50 + '\\nOverall Churn Rate\\n' + '*'*50)\n",
    "# print('TRAIN: ', df_train.status.value_counts(normalize=True)[1].round(4))\n",
    "# print('TEST:  ', df_test.status.value_counts(normalize=True)[1].round(4), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FE for NUMERICAL Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Pipelines by Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Remove_IrrelevantCollinearFeatures' requires target y.\n",
      "'DecisionTreeDiscretizer' requires target y.\n",
      "'Remove_IrrelevantCollinearFeatures' requires target y.\n",
      "'Remove_IrrelevantCollinearFeatures' requires target y.\n",
      "'Remove_IrrelevantCollinearFeatures' requires target y.\n",
      "'Remove_IrrelevantCollinearFeatures' requires target y.\n",
      "'Remove_IrrelevantCollinearFeatures' requires target y.\n",
      "'Remove_IrrelevantCollinearFeatures' requires target y.\n",
      "'Remove_IrrelevantCollinearFeatures' requires target y.\n",
      "'Remove_IrrelevantCollinearFeatures' requires target y.\n",
      "'Remove_IrrelevantCollinearFeatures' requires target y.\n",
      "'Remove_IrrelevantCollinearFeatures' requires target y.\n",
      "'Remove_IrrelevantCollinearFeatures' requires target y.\n",
      "'Remove_IrrelevantCollinearFeatures' requires target y.\n"
     ]
    }
   ],
   "source": [
    "corr_target   = 0.005 # To remove features with low correlation with target: abs(Corr w. Target) < 0.005\n",
    "corr_features = 0.75  # To remove features with high correlation with other features: abs(Corr) > 0.75\n",
    "\n",
    "Pipe_Original    = Pipeline([('Original', FE.PassTransformer(prefix='Original')),\n",
    "                             ('Correlated', FE.Remove_IrrelevantCollinearFeatures(\n",
    "                              corr_threshold_target=corr_target, corr_threshold_features=corr_features))])\n",
    "\n",
    "Pipe_Tree        = Pipeline([('Tree', FE.DecisionTreeDiscretizer(cv=3,  corr_threshold_target=0.001, prefix='Tree_NUM')),\n",
    "                             ('Correlated', FE.Remove_IrrelevantCollinearFeatures(\n",
    "                              corr_threshold_target=corr_target, corr_threshold_features=corr_features))])\n",
    "\n",
    "Pipe_Standard    = Pipeline([('Standard', FE.StandardScaler_DF(prefix='Standard')),\n",
    "                             ('Correlated', FE.Remove_IrrelevantCollinearFeatures(\n",
    "                              corr_threshold_target=corr_target, corr_threshold_features=corr_features))])\n",
    "\n",
    "Pipe_Robust      = Pipeline([('Robust', FE.RobustScaler_DF(prefix='Robust', quantile_range=(5.0, 95.0))),\n",
    "                             ('Correlated', FE.Remove_IrrelevantCollinearFeatures(\n",
    "                              corr_threshold_target=corr_target, corr_threshold_features=corr_features))])\n",
    "\n",
    "Pipe_MinMax      = Pipeline([('MinMax', FE.MinMaxScaler_DF(prefix='MinMax', feature_range=(0, 1))),\n",
    "                             ('Correlated', FE.Remove_IrrelevantCollinearFeatures(\n",
    "                              corr_threshold_target=corr_target, corr_threshold_features=corr_features))])\n",
    "\n",
    "Pipe_MaxAbs     = Pipeline([('MaxAbs', FE.MaxAbsScaler_DF(prefix='MaxAbs')),\n",
    "                             ('Correlated', FE.Remove_IrrelevantCollinearFeatures(\n",
    "                              corr_threshold_target=corr_target, corr_threshold_features=corr_features))])\n",
    "\n",
    "Pipe_Norm       = Pipeline([('Norm', FE.Normalizer_DF(prefix='Norm', norm='l1')),\n",
    "                             ('Correlated', FE.Remove_IrrelevantCollinearFeatures(\n",
    "                              corr_threshold_target=corr_target, corr_threshold_features=corr_features))])\n",
    "\n",
    "Pipe_Quantile   = Pipeline([('Quantile', FE.QuantileTransformer_DF(prefix='Quantile', n_quantiles=100, random_state=0)),\n",
    "                             ('Correlated', FE.Remove_IrrelevantCollinearFeatures(\n",
    "                              corr_threshold_target=corr_target, corr_threshold_features=corr_features))])\n",
    "\n",
    "Pipe_KBin       = Pipeline([('KBin', FE.KBinsDiscretizer_DF(prefix='KBin', n_bins=10, encode='ordinal')),\n",
    "                             ('Correlated', FE.Remove_IrrelevantCollinearFeatures(\n",
    "                              corr_threshold_target=corr_target, corr_threshold_features=corr_features))])\n",
    "\n",
    "Pipe_Binary     = Pipeline([('Binary', FE.Binarizer_DF(prefix='Binary', threshold=0)),\n",
    "                             ('Correlated', FE.Remove_IrrelevantCollinearFeatures(\n",
    "                              corr_threshold_target=corr_target, corr_threshold_features=corr_features))])\n",
    "\n",
    "Pipe_Log1p      = Pipeline([('Log1p', FE.Log1pTransformer(prefix='Log1p')),\n",
    "                             ('Correlated', FE.Remove_IrrelevantCollinearFeatures(\n",
    "                              corr_threshold_target=corr_target, corr_threshold_features=corr_features))])\n",
    "\n",
    "Pipe_Sqrt       = Pipeline([('Sqrt', FE.SqrtTransformer(prefix='Sqrt')),\n",
    "                             ('Correlated', FE.Remove_IrrelevantCollinearFeatures(\n",
    "                              corr_threshold_target=corr_target, corr_threshold_features=corr_features))])\n",
    "\n",
    "Pipe_Reciprocal = Pipeline([('Reciprocal', FE.ReciprocalTransformer(prefix='Reciprocal')),\n",
    "                             ('Correlated', FE.Remove_IrrelevantCollinearFeatures(\n",
    "                              corr_threshold_target=corr_target, corr_threshold_features=corr_features))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create/Use a Meta-Transformer\n",
    "#### abs(correlation ) among Features <= 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Remove_IrrelevantCollinearFeatures' requires target y.\n",
      "\n",
      "****************************************************************************************************\n",
      "Total NUMERICAL Features: 402 features examined\n",
      "Irrelevant Features     : 255 features dropped due to abs(correlation ) <= 0.005 with target\n",
      "Collinear Features      : 32 features dropped due to abs(correlation ) > 0.75 with other features\n",
      "****************************************************************************************************\n",
      "\n",
      "****************************************************************************************************\n",
      "Total NUMERICAL Features: 345 features examined\n",
      "Irrelevant Features     : 115 features dropped due to abs(correlation ) <= 0.005 with target\n",
      "Collinear Features      : 16 features dropped due to abs(correlation ) > 0.75 with other features\n",
      "****************************************************************************************************\n",
      "\n",
      "****************************************************************************************************\n",
      "Total NUMERICAL Features: 402 features examined\n",
      "Irrelevant Features     : 255 features dropped due to abs(correlation ) <= 0.005 with target\n",
      "Collinear Features      : 32 features dropped due to abs(correlation ) > 0.75 with other features\n",
      "****************************************************************************************************\n",
      "\n",
      "****************************************************************************************************\n",
      "Total NUMERICAL Features: 402 features examined\n",
      "Irrelevant Features     : 255 features dropped due to abs(correlation ) <= 0.005 with target\n",
      "Collinear Features      : 32 features dropped due to abs(correlation ) > 0.75 with other features\n",
      "****************************************************************************************************\n",
      "\n",
      "****************************************************************************************************\n",
      "Total NUMERICAL Features: 402 features examined\n",
      "Irrelevant Features     : 240 features dropped due to abs(correlation ) <= 0.005 with target\n",
      "Collinear Features      : 45 features dropped due to abs(correlation ) > 0.75 with other features\n",
      "****************************************************************************************************\n",
      "\n",
      "****************************************************************************************************\n",
      "Total NUMERICAL Features: 402 features examined\n",
      "Irrelevant Features     : 262 features dropped due to abs(correlation ) <= 0.005 with target\n",
      "Collinear Features      : 36 features dropped due to abs(correlation ) > 0.75 with other features\n",
      "****************************************************************************************************\n",
      "\n",
      "****************************************************************************************************\n",
      "Total NUMERICAL Features: 402 features examined\n",
      "Irrelevant Features     : 255 features dropped due to abs(correlation ) <= 0.005 with target\n",
      "Collinear Features      : 32 features dropped due to abs(correlation ) > 0.75 with other features\n",
      "****************************************************************************************************\n",
      "\n",
      "****************************************************************************************************\n",
      "Total NUMERICAL Features: 402 features examined\n",
      "Irrelevant Features     : 255 features dropped due to abs(correlation ) <= 0.005 with target\n",
      "Collinear Features      : 32 features dropped due to abs(correlation ) > 0.75 with other features\n",
      "****************************************************************************************************\n",
      "\n",
      "****************************************************************************************************\n",
      "Total NUMERICAL Features: 402 features examined\n",
      "Irrelevant Features     : 295 features dropped due to abs(correlation ) <= 0.005 with target\n",
      "Collinear Features      : 38 features dropped due to abs(correlation ) > 0.75 with other features\n",
      "****************************************************************************************************\n",
      "\n",
      "****************************************************************************************************\n",
      "Total NUMERICAL Features: 402 features examined\n",
      "Irrelevant Features     : 291 features dropped due to abs(correlation ) <= 0.005 with target\n",
      "Collinear Features      : 40 features dropped due to abs(correlation ) > 0.75 with other features\n",
      "****************************************************************************************************\n",
      "\n",
      "****************************************************************************************************\n",
      "Total NUMERICAL Features: 402 features examined\n",
      "Irrelevant Features     : 263 features dropped due to abs(correlation ) <= 0.005 with target\n",
      "Collinear Features      : 45 features dropped due to abs(correlation ) > 0.75 with other features\n",
      "****************************************************************************************************\n",
      "\n",
      "****************************************************************************************************\n",
      "Total NUMERICAL Features: 402 features examined\n",
      "Irrelevant Features     : 238 features dropped due to abs(correlation ) <= 0.005 with target\n",
      "Collinear Features      : 48 features dropped due to abs(correlation ) > 0.75 with other features\n",
      "****************************************************************************************************\n",
      "\n",
      "****************************************************************************************************\n",
      "Total NUMERICAL Features: 402 features examined\n",
      "Irrelevant Features     : 305 features dropped due to abs(correlation ) <= 0.005 with target\n",
      "Collinear Features      : 8 features dropped due to abs(correlation ) > 0.75 with other features\n",
      "****************************************************************************************************\n",
      "\n",
      "****************************************************************************************************\n",
      "Total NUMERICAL Features: 1449 features examined\n",
      "Irrelevant Features     : 873 features dropped due to abs(correlation ) <= 0.01 with target\n",
      "Collinear Features      : 370 features dropped due to abs(correlation ) > 0.75 with other features\n",
      "****************************************************************************************************\n",
      "\n",
      "**************************************************\n",
      "Before vs After Feature Engineering (FE)\n",
      "**************************************************\n",
      "TRAIN: Before FE:(106176, 789)\n",
      "TRAIN: After FE: (106176, 206)\n",
      "TEST:  After FE: (102062, 206)\n",
      "CPU times: user 10min 50s, sys: 36.5 s, total: 11min 26s\n",
      "Wall time: 11min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corr_target   = 0.01  # To remove features with low correlation with target: abs(Corr w. Target) < 0.01\n",
    "corr_features = 0.75  # To remove features with high correlation with other features: abs(Corr) > 0.75\n",
    "\n",
    "# (1) Make a Pipeline in Parallel/Sequence and Instantiate \n",
    "# Custom Transformers in Parallel for NUMERICAL Features\n",
    "Pipe_FU          =  FE.FeatureUnion_DF([\n",
    "                                        ('Original_Pipe', Pipe_Original),     \n",
    "                                        ('Tree_Pipe', Pipe_Tree),         \n",
    "                                        ('Standard_Pipe', Pipe_Standard), \n",
    "                                        ('Robust_Pipe', Pipe_Robust), \n",
    "                                        ('Quantile_Pipe', Pipe_Quantile), \n",
    "                                        ('Binary_Pipe', Pipe_Binary),         \n",
    "                                        ('MinMax_Pipe', Pipe_MinMax), \n",
    "                                        ('MaxAbs_Pipe', Pipe_MaxAbs),                                                                                                                                \n",
    "                                        ('Norm_Pipe', Pipe_Norm), \n",
    "                                        ('KBin', Pipe_KBin),     \n",
    "                                        ('Log1p_Pipe', Pipe_Log1p), \n",
    "                                        ('Sqrt_Pipe', Pipe_Sqrt), \n",
    "                                        ('Reciprocal_Pipe', Pipe_Reciprocal)\n",
    "                                      ])\n",
    "# Custom Transformers in Sequence for NUMERICAL Features\n",
    "NUM_Pipe          = Pipeline([\n",
    "                              ('Selector', FE.FeatureSelector_NUM()),\n",
    "                              ('FU_Pipe', Pipe_FU),\n",
    "                              ('IrrelevantCollinear', FE.Remove_IrrelevantCollinearFeatures(\n",
    "                               corr_threshold_target=corr_target, corr_threshold_features=corr_features))\n",
    "                             ])\n",
    "\n",
    "\n",
    "# (2) fit()\n",
    "NUM_Pipe.fit(train_X, train_y)\n",
    "\n",
    "\n",
    "# (3) transform()\n",
    "train_X_FE = NUM_Pipe.transform(train_X)\n",
    "test_X_FE  = NUM_Pipe.transform(test_X)\n",
    "\n",
    "# Feature Dimension\n",
    "print('\\n' + '*'*50 + '\\nBefore vs After Feature Engineering (FE)\\n' + '*'*50)\n",
    "print('TRAIN: Before FE:' + str(train_X.shape))\n",
    "print('TRAIN: After FE: ' + str(train_X_FE.shape))\n",
    "print('TEST:  After FE: ' + str(test_X_FE.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**************************************************\n",
      "Correlation Summary: TRAIN vs TEST\n",
      "**************************************************\n",
      "abs(correlation) among Features <= 0.75\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRAIN_Original</th>\n",
       "      <th>TRAIN_All</th>\n",
       "      <th>TEST_Original</th>\n",
       "      <th>TEST_All</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>402.000000</td>\n",
       "      <td>206.000000</td>\n",
       "      <td>402.000000</td>\n",
       "      <td>206.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000722</td>\n",
       "      <td>0.016547</td>\n",
       "      <td>0.001845</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.008148</td>\n",
       "      <td>0.015291</td>\n",
       "      <td>0.009021</td>\n",
       "      <td>0.013968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.036538</td>\n",
       "      <td>-0.022044</td>\n",
       "      <td>-0.045534</td>\n",
       "      <td>-0.027004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1%</th>\n",
       "      <td>-0.023006</td>\n",
       "      <td>-0.018947</td>\n",
       "      <td>-0.017500</td>\n",
       "      <td>-0.019239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5%</th>\n",
       "      <td>-0.010472</td>\n",
       "      <td>-0.014204</td>\n",
       "      <td>-0.007537</td>\n",
       "      <td>-0.007335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10%</th>\n",
       "      <td>-0.006377</td>\n",
       "      <td>-0.010886</td>\n",
       "      <td>-0.005932</td>\n",
       "      <td>-0.003594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20%</th>\n",
       "      <td>-0.004024</td>\n",
       "      <td>0.010757</td>\n",
       "      <td>-0.003819</td>\n",
       "      <td>-0.000497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30%</th>\n",
       "      <td>-0.002742</td>\n",
       "      <td>0.012620</td>\n",
       "      <td>-0.002410</td>\n",
       "      <td>0.001682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40%</th>\n",
       "      <td>-0.001234</td>\n",
       "      <td>0.014707</td>\n",
       "      <td>-0.001070</td>\n",
       "      <td>0.004006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.016181</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.006773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60%</th>\n",
       "      <td>0.001367</td>\n",
       "      <td>0.019052</td>\n",
       "      <td>0.001757</td>\n",
       "      <td>0.009455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70%</th>\n",
       "      <td>0.003150</td>\n",
       "      <td>0.022425</td>\n",
       "      <td>0.003143</td>\n",
       "      <td>0.016025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80%</th>\n",
       "      <td>0.005519</td>\n",
       "      <td>0.027503</td>\n",
       "      <td>0.005839</td>\n",
       "      <td>0.022926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90%</th>\n",
       "      <td>0.010323</td>\n",
       "      <td>0.035042</td>\n",
       "      <td>0.014177</td>\n",
       "      <td>0.027229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95%</th>\n",
       "      <td>0.015351</td>\n",
       "      <td>0.039896</td>\n",
       "      <td>0.021118</td>\n",
       "      <td>0.034764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99%</th>\n",
       "      <td>0.024172</td>\n",
       "      <td>0.046854</td>\n",
       "      <td>0.028562</td>\n",
       "      <td>0.048345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.027573</td>\n",
       "      <td>0.062536</td>\n",
       "      <td>0.035955</td>\n",
       "      <td>0.052710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       TRAIN_Original   TRAIN_All  TEST_Original    TEST_All\n",
       "count      402.000000  206.000000     402.000000  206.000000\n",
       "mean         0.000722    0.016547       0.001845    0.010000\n",
       "std          0.008148    0.015291       0.009021    0.013968\n",
       "min         -0.036538   -0.022044      -0.045534   -0.027004\n",
       "1%          -0.023006   -0.018947      -0.017500   -0.019239\n",
       "5%          -0.010472   -0.014204      -0.007537   -0.007335\n",
       "10%         -0.006377   -0.010886      -0.005932   -0.003594\n",
       "20%         -0.004024    0.010757      -0.003819   -0.000497\n",
       "30%         -0.002742    0.012620      -0.002410    0.001682\n",
       "40%         -0.001234    0.014707      -0.001070    0.004006\n",
       "50%          0.000106    0.016181       0.000190    0.006773\n",
       "60%          0.001367    0.019052       0.001757    0.009455\n",
       "70%          0.003150    0.022425       0.003143    0.016025\n",
       "80%          0.005519    0.027503       0.005839    0.022926\n",
       "90%          0.010323    0.035042       0.014177    0.027229\n",
       "95%          0.015351    0.039896       0.021118    0.034764\n",
       "99%          0.024172    0.046854       0.028562    0.048345\n",
       "max          0.027573    0.062536       0.035955    0.052710"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_list          = [.01, .05, .1, .2, .3, .4, .6, .7, .8, .9, .95, .99]\n",
    "flag_NUM        = train_X.select_dtypes(exclude=[object, 'category']).columns.tolist()\n",
    "corr_train      = train_X[flag_NUM].apply(lambda x: x.corr(train_y)).to_frame().describe(percentiles=p_list)\n",
    "corr_train_all  = train_X_FE.apply(lambda x: x.corr(train_y)).to_frame().describe(percentiles=p_list)\n",
    "\n",
    "corr_test       = test_X[flag_NUM].apply(lambda x: x.corr(test_y)).to_frame().describe(percentiles=p_list)\n",
    "corr_test_all   = test_X_FE.apply(lambda x: x.corr(test_y)).to_frame().describe(percentiles=p_list)\n",
    "\n",
    "corr_all         = pd.concat([corr_train, corr_train_all, corr_test, corr_test_all], axis=1)\n",
    "corr_all.columns = ['TRAIN_Original', 'TRAIN_All', 'TEST_Original', 'TEST_All']\n",
    "print('\\n' + '*'*50 + '\\nCorrelation Summary: TRAIN vs TEST\\n' + '*'*50)\n",
    "print('abs(correlation) among Features <= 0.75')\n",
    "corr_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_FE_NUM = train_y.to_frame().\\\n",
    "                 merge(train_X_FE, how='inner', left_index=True, right_index=True)\n",
    "\n",
    "df_test_FE_NUM  = test_y.to_frame().\\\n",
    "                  merge(test_X_FE, how='inner', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### abs(correlation ) among Features <= 0.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Remove_IrrelevantCollinearFeatures' requires target y.\n",
      "\n",
      "****************************************************************************************************\n",
      "Total NUMERICAL Features: 206 features examined\n",
      "Irrelevant Features     : 0 features dropped due to abs(correlation ) <= 0.01 with target\n",
      "Collinear Features      : 89 features dropped due to abs(correlation ) > 0.5 with other features\n",
      "****************************************************************************************************\n",
      "\n",
      "**************************************************\n",
      "Before vs After Feature Engineering (FE)\n",
      "**************************************************\n",
      "TRAIN: Before FE:(106176, 206)\n",
      "TRAIN: After FE: (106176, 117)\n",
      "TEST:  After FE: (102062, 117)\n"
     ]
    }
   ],
   "source": [
    "# corr_target   = 0.01  # To remove features with low correlation with target: abs(Corr w. Target) < 0.01\n",
    "# corr_features = 0.50  # To remove features with high correlation with other features: abs(Corr) > 0.50\n",
    "\n",
    "# (1) Instantiate\n",
    "RICF = FE.Remove_IrrelevantCollinearFeatures(corr_threshold_target=0.01, corr_threshold_features=0.50)\n",
    "\n",
    "# (2) Fit\n",
    "RICF.fit(train_X_FE, train_y)\n",
    "\n",
    "# (3) Transform\n",
    "train_X_FE_50Corr = RICF.transform(train_X_FE)\n",
    "test_X_FE_50Corr  = RICF.transform(test_X_FE)\n",
    "\n",
    "# Feature Dimension\n",
    "print('\\n' + '*'*50 + '\\nBefore vs After Feature Engineering (FE)' + '\\n' + '*'*50)\n",
    "print('TRAIN: Before FE:' + str(train_X_FE.shape))\n",
    "print('TRAIN: After FE: ' + str(train_X_FE_50Corr.shape))\n",
    "print('TEST:  After FE: ' + str(test_X_FE_50Corr.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**************************************************\n",
      "Correlation Summary: TRAIN vs TEST\n",
      "**************************************************\n",
      "abs(correlation ) among Features <= 0.50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRAIN_Original</th>\n",
       "      <th>TRAIN_All</th>\n",
       "      <th>TEST_Original</th>\n",
       "      <th>TEST_All</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>402.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>402.000000</td>\n",
       "      <td>117.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000722</td>\n",
       "      <td>0.016283</td>\n",
       "      <td>0.001845</td>\n",
       "      <td>0.006824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.008148</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>0.009021</td>\n",
       "      <td>0.012284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.036538</td>\n",
       "      <td>-0.022044</td>\n",
       "      <td>-0.045534</td>\n",
       "      <td>-0.023111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1%</th>\n",
       "      <td>-0.023006</td>\n",
       "      <td>-0.017671</td>\n",
       "      <td>-0.017500</td>\n",
       "      <td>-0.019110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5%</th>\n",
       "      <td>-0.010472</td>\n",
       "      <td>-0.012448</td>\n",
       "      <td>-0.007537</td>\n",
       "      <td>-0.010099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10%</th>\n",
       "      <td>-0.006377</td>\n",
       "      <td>-0.010599</td>\n",
       "      <td>-0.005932</td>\n",
       "      <td>-0.003322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20%</th>\n",
       "      <td>-0.004024</td>\n",
       "      <td>0.010747</td>\n",
       "      <td>-0.003819</td>\n",
       "      <td>-0.000804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30%</th>\n",
       "      <td>-0.002742</td>\n",
       "      <td>0.012595</td>\n",
       "      <td>-0.002410</td>\n",
       "      <td>0.000461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40%</th>\n",
       "      <td>-0.001234</td>\n",
       "      <td>0.014208</td>\n",
       "      <td>-0.001070</td>\n",
       "      <td>0.002791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.015927</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.004447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60%</th>\n",
       "      <td>0.001367</td>\n",
       "      <td>0.017674</td>\n",
       "      <td>0.001757</td>\n",
       "      <td>0.007376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70%</th>\n",
       "      <td>0.003150</td>\n",
       "      <td>0.021491</td>\n",
       "      <td>0.003143</td>\n",
       "      <td>0.009067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80%</th>\n",
       "      <td>0.005519</td>\n",
       "      <td>0.025362</td>\n",
       "      <td>0.005839</td>\n",
       "      <td>0.013042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90%</th>\n",
       "      <td>0.010323</td>\n",
       "      <td>0.033958</td>\n",
       "      <td>0.014177</td>\n",
       "      <td>0.021755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95%</th>\n",
       "      <td>0.015351</td>\n",
       "      <td>0.040895</td>\n",
       "      <td>0.021118</td>\n",
       "      <td>0.031827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99%</th>\n",
       "      <td>0.024172</td>\n",
       "      <td>0.048952</td>\n",
       "      <td>0.028562</td>\n",
       "      <td>0.043974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.027573</td>\n",
       "      <td>0.062536</td>\n",
       "      <td>0.035955</td>\n",
       "      <td>0.052710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       TRAIN_Original   TRAIN_All  TEST_Original    TEST_All\n",
       "count      402.000000  117.000000     402.000000  117.000000\n",
       "mean         0.000722    0.016283       0.001845    0.006824\n",
       "std          0.008148    0.014900       0.009021    0.012284\n",
       "min         -0.036538   -0.022044      -0.045534   -0.023111\n",
       "1%          -0.023006   -0.017671      -0.017500   -0.019110\n",
       "5%          -0.010472   -0.012448      -0.007537   -0.010099\n",
       "10%         -0.006377   -0.010599      -0.005932   -0.003322\n",
       "20%         -0.004024    0.010747      -0.003819   -0.000804\n",
       "30%         -0.002742    0.012595      -0.002410    0.000461\n",
       "40%         -0.001234    0.014208      -0.001070    0.002791\n",
       "50%          0.000106    0.015927       0.000190    0.004447\n",
       "60%          0.001367    0.017674       0.001757    0.007376\n",
       "70%          0.003150    0.021491       0.003143    0.009067\n",
       "80%          0.005519    0.025362       0.005839    0.013042\n",
       "90%          0.010323    0.033958       0.014177    0.021755\n",
       "95%          0.015351    0.040895       0.021118    0.031827\n",
       "99%          0.024172    0.048952       0.028562    0.043974\n",
       "max          0.027573    0.062536       0.035955    0.052710"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_list          = [.01, .05, .1, .2, .3, .4, .6, .7, .8, .9, .95, .99]\n",
    "flag_NUM        = train_X.select_dtypes(exclude=[object, 'category']).columns.tolist()\n",
    "corr_train      = train_X[flag_NUM].apply(lambda x: x.corr(train_y)).to_frame().describe(percentiles=p_list)\n",
    "corr_train_all  = train_X_FE_50Corr.apply(lambda x: x.corr(train_y)).to_frame().describe(percentiles=p_list)\n",
    "\n",
    "corr_test       = test_X[flag_NUM].apply(lambda x: x.corr(test_y)).to_frame().describe(percentiles=p_list)\n",
    "corr_test_all   = test_X_FE_50Corr.apply(lambda x: x.corr(test_y)).to_frame().describe(percentiles=p_list)\n",
    "\n",
    "corr_all         = pd.concat([corr_train, corr_train_all, corr_test, corr_test_all], axis=1)\n",
    "corr_all.columns = ['TRAIN_Original', 'TRAIN_All', 'TEST_Original', 'TEST_All']\n",
    "print('\\n' + '*'*50 + '\\nCorrelation Summary: TRAIN vs TEST\\n' + '*'*50)\n",
    "print('abs(correlation) among Features <= 0.50')\n",
    "corr_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_FE_NUM_50Corr = train_y.to_frame().\\\n",
    "                         merge(train_X_FE_50Corr, how='inner', left_index=True, right_index=True)\n",
    "\n",
    "df_test_FE_NUM_50Corr  = test_y.to_frame().\\\n",
    "                         merge(test_X_FE_50Corr, how='inner', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FE for CATEGORICAL Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Pipelines by Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Remove_IrrelevantCollinearFeatures' requires target y.\n",
      "'Remove_IrrelevantCollinearFeatures' requires target y.\n",
      "'Remove_IrrelevantCollinearFeatures' requires target y.\n",
      "'ordinal' encoding requires target y.\n",
      "'Remove_IrrelevantCollinearFeatures' requires target y.\n",
      "'y_mean' encoding requires target y.\n",
      "'Remove_IrrelevantCollinearFeatures' requires target y.\n",
      "'y_log_ratio' encoding requires target y.\n",
      "'Remove_IrrelevantCollinearFeatures' requires target y.\n",
      "'y_ratio' encoding requires target y.\n",
      "'Remove_IrrelevantCollinearFeatures' requires target y.\n",
      "'FeatureAggregator' requires target y.\n",
      "'Remove_IrrelevantCollinearFeatures' requires target y.\n",
      "'ordinal' encoding requires target y.\n",
      "'DecisionTreeDiscretizer' requires target y.\n",
      "'Remove_IrrelevantCollinearFeatures' requires target y.\n"
     ]
    }
   ],
   "source": [
    "corr_target   = 0.005 # To remove features with low correlation with target: abs(Corr w. Target) < 0.005\n",
    "corr_features = 0.75  # To remove features with high correlation with other features: abs(Corr) > 0.75\n",
    "\n",
    "fe_1st           = ['grp_tenure_3m', 'grp_payment_method', \\\n",
    "                    'grp_payment_25dollar', 'grp_payment_change_10dollar', 'grp_payment_change_5pct', \\\n",
    "                    'grp_payment_income', 'grp_call_csc', 'grp_call_bill', \\\n",
    "                    'grp_call_csr', 'grp_call_tsr']\n",
    "fe_2nd           = fe_1st + ['income_demos', 'ethnic', 'age_demos', 'archetype']\n",
    "fe_group         = ['census', 'cleansed_city', 'trunk', 'hub']\n",
    "\n",
    "Pipe_OHE         = Pipeline([('OHE', FE.UniversalCategoryEncoder(encoding_method='ohe')),\n",
    "                             ('Correlated', FE.Remove_IrrelevantCollinearFeatures(\n",
    "                              corr_threshold_target=corr_target, corr_threshold_features=corr_features))])\n",
    "\n",
    "Pipe_PCT         = Pipeline([('PCT', FE.UniversalCategoryEncoder(encoding_method='pct', prefix='PCT')),\n",
    "                             ('Correlated', FE.Remove_IrrelevantCollinearFeatures(\n",
    "                              corr_threshold_target=corr_target, corr_threshold_features=corr_features))])\n",
    "\n",
    "Pipe_COUNT       = Pipeline([('COUNT', FE.UniversalCategoryEncoder(encoding_method='count', prefix='COUNT')),\n",
    "                             ('Correlated', FE.Remove_IrrelevantCollinearFeatures(\n",
    "                              corr_threshold_target=corr_target, corr_threshold_features=corr_features))])\n",
    "\n",
    "Pipe_ORDINAL     = Pipeline([('ORDINAL', FE.UniversalCategoryEncoder(encoding_method='ordinal', prefix='ORDINAL')),\n",
    "                             ('Correlated', FE.Remove_IrrelevantCollinearFeatures(\n",
    "                              corr_threshold_target=corr_target, corr_threshold_features=corr_features))])\n",
    "\n",
    "Pipe_Y_MEAN      = Pipeline([('Y_MEAN', FE.UniversalCategoryEncoder(encoding_method='y_mean', prefix='Y_MEAN')),\n",
    "                             ('Correlated', FE.Remove_IrrelevantCollinearFeatures(\n",
    "                              corr_threshold_target=corr_target, corr_threshold_features=corr_features))])\n",
    "\n",
    "Pipe_Y_LOG_RATIO = Pipeline([('Y_LOG_RATIO', FE.UniversalCategoryEncoder(encoding_method='y_log_ratio', prefix='Y_LOG_RATIO')),\n",
    "                             ('Correlated', FE.Remove_IrrelevantCollinearFeatures(\n",
    "                              corr_threshold_target=corr_target, corr_threshold_features=corr_features))])\n",
    "\n",
    "Pipe_Y_RATIO     = Pipeline([('Y_RATIO', FE.UniversalCategoryEncoder(encoding_method='y_ratio', prefix='Y_RATIO')),\n",
    "                             ('Correlated', FE.Remove_IrrelevantCollinearFeatures(\n",
    "                              corr_threshold_target=corr_target, corr_threshold_features=corr_features))])\n",
    "\n",
    "Pipe_Aggregation = Pipeline([('Aggregation', FE.FeatureAggregator(features_grouping=fe_group, correlation_threshold=0.01)),\n",
    "                             ('Correlated', FE.Remove_IrrelevantCollinearFeatures(\n",
    "                              corr_threshold_target=corr_target, corr_threshold_features=corr_features))])\n",
    "\n",
    "Pipe_Tree        = Pipeline([('ORDINAL', FE.UniversalCategoryEncoder(encoding_method='ordinal')),\n",
    "                             ('Tree', FE.DecisionTreeDiscretizer(cv=3,  corr_threshold_target=0.001, prefix='Tree_CAT')),\n",
    "                             ('Correlated', FE.Remove_IrrelevantCollinearFeatures(\n",
    "                              corr_threshold_target=corr_target, corr_threshold_features=corr_features))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create/Use a Meta-Transformer\n",
    "#### abs(correlation ) among Features <= 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Remove_IrrelevantCollinearFeatures' requires target y.\n",
      "\n",
      "****************************************************************************************************\n",
      "Total NUMERICAL Features: 3815 features examined\n",
      "Irrelevant Features     : 2260 features dropped due to abs(correlation ) <= 0.005 with target\n",
      "Collinear Features      : 887 features dropped due to abs(correlation ) > 0.75 with other features\n",
      "****************************************************************************************************\n",
      "\n",
      "****************************************************************************************************\n",
      "Total NUMERICAL Features: 464 features examined\n",
      "Irrelevant Features     : 227 features dropped due to abs(correlation ) <= 0.005 with target\n",
      "Collinear Features      : 147 features dropped due to abs(correlation ) > 0.75 with other features\n",
      "****************************************************************************************************\n",
      "\n",
      "****************************************************************************************************\n",
      "Total NUMERICAL Features: 464 features examined\n",
      "Irrelevant Features     : 227 features dropped due to abs(correlation ) <= 0.005 with target\n",
      "Collinear Features      : 147 features dropped due to abs(correlation ) > 0.75 with other features\n",
      "****************************************************************************************************\n",
      "\n",
      "****************************************************************************************************\n",
      "Total NUMERICAL Features: 464 features examined\n",
      "Irrelevant Features     : 133 features dropped due to abs(correlation ) <= 0.005 with target\n",
      "Collinear Features      : 195 features dropped due to abs(correlation ) > 0.75 with other features\n",
      "****************************************************************************************************\n",
      "\n",
      "****************************************************************************************************\n",
      "Total NUMERICAL Features: 464 features examined\n",
      "Irrelevant Features     : 129 features dropped due to abs(correlation ) <= 0.005 with target\n",
      "Collinear Features      : 194 features dropped due to abs(correlation ) > 0.75 with other features\n",
      "****************************************************************************************************\n",
      "\n",
      "****************************************************************************************************\n",
      "Total NUMERICAL Features: 464 features examined\n",
      "Irrelevant Features     : 130 features dropped due to abs(correlation ) <= 0.005 with target\n",
      "Collinear Features      : 185 features dropped due to abs(correlation ) > 0.75 with other features\n",
      "****************************************************************************************************\n",
      "\n",
      "****************************************************************************************************\n",
      "Total NUMERICAL Features: 464 features examined\n",
      "Irrelevant Features     : 129 features dropped due to abs(correlation ) <= 0.005 with target\n",
      "Collinear Features      : 194 features dropped due to abs(correlation ) > 0.75 with other features\n",
      "****************************************************************************************************\n",
      "\n",
      "****************************************************************************************************\n",
      "Total NUMERICAL Features: 369 features examined\n",
      "Irrelevant Features     : 0 features dropped due to abs(correlation ) <= 0.005 with target\n",
      "Collinear Features      : 280 features dropped due to abs(correlation ) > 0.75 with other features\n",
      "****************************************************************************************************\n",
      "\n",
      "****************************************************************************************************\n",
      "Total NUMERICAL Features: 422 features examined\n",
      "Irrelevant Features     : 101 features dropped due to abs(correlation ) <= 0.005 with target\n",
      "Collinear Features      : 180 features dropped due to abs(correlation ) > 0.75 with other features\n",
      "****************************************************************************************************\n",
      "\n",
      "****************************************************************************************************\n",
      "Total NUMERICAL Features: 1645 features examined\n",
      "Irrelevant Features     : 717 features dropped due to abs(correlation ) <= 0.01 with target\n",
      "Collinear Features      : 537 features dropped due to abs(correlation ) > 0.75 with other features\n",
      "****************************************************************************************************\n",
      "\n",
      "**************************************************\n",
      "Before vs After Feature Engineering (FE)\n",
      "**************************************************\n",
      "TRAIN: Before FE:(106176, 789)\n",
      "TRAIN: After FE: (106176, 391)\n",
      "TEST:  After FE: (102062, 391)\n",
      "CPU times: user 1h 45min 28s, sys: 45min 56s, total: 2h 31min 24s\n",
      "Wall time: 2h 31min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corr_target   = 0.01  # To remove features with low correlation with target: abs(Corr w. Target) < 0.01\n",
    "corr_features = 0.75  # To remove features with high correlation with other features: abs(Corr) > 0.75\n",
    "\n",
    "# (1) Make a Pipeline in Parallel/Sequence and Instantiate \n",
    "# Custom Transformers in Parallel for CATEGORICAL Features\n",
    "Pipe_FU          =  FE.FeatureUnion_DF([\n",
    "                    ('OHE_Pipe', Pipe_OHE),\n",
    "                    ('PCT_Pipe', Pipe_PCT),\n",
    "                    ('COUNT_Pipe', Pipe_COUNT),\n",
    "                    ('ORDINAL_Pipe', Pipe_ORDINAL),\n",
    "                    ('Y_MEAN_Pipe', Pipe_Y_MEAN),\n",
    "                    ('Y_LOG_RATIO_Pipe', Pipe_Y_LOG_RATIO),\n",
    "                    ('Y_RATIO_Pipe', Pipe_Y_RATIO),\n",
    "                    ('Aggregation_Pipe', Pipe_Aggregation),\n",
    "                    ('Tree_Pipe', Pipe_Tree)\n",
    "                    ])\n",
    "\n",
    "# Custom Transformers in Sequence for CATEGORICAL Features\n",
    "CAT_Pipe          = Pipeline([\n",
    "                    ('Interaction', FE.FeatureInteractionTransformer(features_1st=fe_1st, features_2nd=fe_2nd)),\n",
    "                    ('RareCategory', FE.RareCategoryEncoder(category_min_pct=0.01, category_max_count=30)),\n",
    "                    ('FU_Pipe', Pipe_FU),\n",
    "                    ('IrrelevantCollinear', FE.Remove_IrrelevantCollinearFeatures(\n",
    "                     corr_threshold_target=corr_target, corr_threshold_features=corr_features))\n",
    "                    ])\n",
    "\n",
    "\n",
    "# (2) fit()\n",
    "CAT_Pipe.fit(train_X, train_y)\n",
    "\n",
    "\n",
    "# (3) transform()\n",
    "train_X_FE = CAT_Pipe.transform(train_X)\n",
    "test_X_FE  = CAT_Pipe.transform(test_X)\n",
    "\n",
    "# Feature Dimension\n",
    "print('\\n' + '*'*50 + '\\nBefore vs After Feature Engineering (FE)\\n' + '*'*50)\n",
    "print('TRAIN: Before FE:' + str(train_X.shape))\n",
    "print('TRAIN: After FE: ' + str(train_X_FE.shape))\n",
    "print('TEST:  After FE: ' + str(test_X_FE.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**************************************************\n",
      "Correlation Summary: TRAIN vs TEST\n",
      "**************************************************\n",
      "abs(correlation ) among Features <= 0.75\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRAIN_All</th>\n",
       "      <th>TEST_All</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>391.000000</td>\n",
       "      <td>386.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.010214</td>\n",
       "      <td>0.007751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.014178</td>\n",
       "      <td>0.014186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.024233</td>\n",
       "      <td>-0.040173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1%</th>\n",
       "      <td>-0.020414</td>\n",
       "      <td>-0.022277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5%</th>\n",
       "      <td>-0.014528</td>\n",
       "      <td>-0.014605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10%</th>\n",
       "      <td>-0.012842</td>\n",
       "      <td>-0.010307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20%</th>\n",
       "      <td>-0.010225</td>\n",
       "      <td>-0.002554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30%</th>\n",
       "      <td>0.010540</td>\n",
       "      <td>0.000745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40%</th>\n",
       "      <td>0.011267</td>\n",
       "      <td>0.004052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.012402</td>\n",
       "      <td>0.007385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60%</th>\n",
       "      <td>0.013795</td>\n",
       "      <td>0.010369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70%</th>\n",
       "      <td>0.015876</td>\n",
       "      <td>0.013598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80%</th>\n",
       "      <td>0.019079</td>\n",
       "      <td>0.017698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90%</th>\n",
       "      <td>0.025586</td>\n",
       "      <td>0.025736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95%</th>\n",
       "      <td>0.030367</td>\n",
       "      <td>0.031274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99%</th>\n",
       "      <td>0.039495</td>\n",
       "      <td>0.048179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.052060</td>\n",
       "      <td>0.056718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        TRAIN_All    TEST_All\n",
       "count  391.000000  386.000000\n",
       "mean     0.010214    0.007751\n",
       "std      0.014178    0.014186\n",
       "min     -0.024233   -0.040173\n",
       "1%      -0.020414   -0.022277\n",
       "5%      -0.014528   -0.014605\n",
       "10%     -0.012842   -0.010307\n",
       "20%     -0.010225   -0.002554\n",
       "30%      0.010540    0.000745\n",
       "40%      0.011267    0.004052\n",
       "50%      0.012402    0.007385\n",
       "60%      0.013795    0.010369\n",
       "70%      0.015876    0.013598\n",
       "80%      0.019079    0.017698\n",
       "90%      0.025586    0.025736\n",
       "95%      0.030367    0.031274\n",
       "99%      0.039495    0.048179\n",
       "max      0.052060    0.056718"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_list          = [.01, .05, .1, .2, .3, .4, .6, .7, .8, .9, .95, .99]\n",
    "corr_train_all  = train_X_FE.apply(lambda x: x.corr(train_y)).to_frame().describe(percentiles=p_list)\n",
    "corr_test_all   = test_X_FE.apply(lambda x: x.corr(test_y)).to_frame().describe(percentiles=p_list)\n",
    "\n",
    "corr_all         = pd.concat([corr_train_all, corr_test_all], axis=1)\n",
    "corr_all.columns = ['TRAIN_All', 'TEST_All']\n",
    "print('\\n' + '*'*50 + '\\nCorrelation Summary: TRAIN vs TEST\\n' + '*'*50)\n",
    "print('abs(correlation) among Features <= 0.75')\n",
    "corr_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### abs(correlation ) among Features <= 0.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Remove_IrrelevantCollinearFeatures' requires target y.\n",
      "\n",
      "****************************************************************************************************\n",
      "Total NUMERICAL Features: 391 features examined\n",
      "Irrelevant Features     : 0 features dropped due to abs(correlation ) <= 0.01 with target\n",
      "Collinear Features      : 215 features dropped due to abs(correlation ) > 0.5 with other features\n",
      "****************************************************************************************************\n",
      "\n",
      "**************************************************\n",
      "Before vs After Feature Engineering (FE)**************************************************\n",
      "TRAIN: Before FE:(106176, 391)\n",
      "TRAIN: After FE: (106176, 176)\n",
      "TEST:  After FE: (102062, 176)\n"
     ]
    }
   ],
   "source": [
    "# corr_target   = 0.01  # To remove features with low correlation with target: abs(Corr w. Target) < 0.01\n",
    "# corr_features = 0.50  # To remove features with high correlation with other features: abs(Corr) > 0.50\n",
    "\n",
    "# (1) Instantiate\n",
    "RICF = FE.Remove_IrrelevantCollinearFeatures(corr_threshold_target=0.01, corr_threshold_features=0.50)\n",
    "\n",
    "# (2) Fit\n",
    "RICF.fit(train_X_FE, train_y)\n",
    "\n",
    "# (3) Transform\n",
    "train_X_FE_50Corr = RICF.transform(train_X_FE)\n",
    "test_X_FE_50Corr  = RICF.transform(test_X_FE)\n",
    "\n",
    "# Feature Dimension\n",
    "print('\\n' + '*'*50 + '\\nBefore vs After Feature Engineering (FE)' + '*'*50)\n",
    "print('TRAIN: Before FE:' + str(train_X_FE.shape))\n",
    "print('TRAIN: After FE: ' + str(train_X_FE_50Corr.shape))\n",
    "print('TEST:  After FE: ' + str(test_X_FE_50Corr.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**************************************************\n",
      "Correlation Summary: TRAIN vs TEST\n",
      "**************************************************\n",
      "abs(correlation ) among Features <= 0.50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRAIN_All</th>\n",
       "      <th>TEST_All</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>176.000000</td>\n",
       "      <td>173.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.009022</td>\n",
       "      <td>0.005629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.014403</td>\n",
       "      <td>0.012550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.021032</td>\n",
       "      <td>-0.040173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1%</th>\n",
       "      <td>-0.017761</td>\n",
       "      <td>-0.022310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5%</th>\n",
       "      <td>-0.014766</td>\n",
       "      <td>-0.013395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10%</th>\n",
       "      <td>-0.013404</td>\n",
       "      <td>-0.010332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20%</th>\n",
       "      <td>-0.010516</td>\n",
       "      <td>-0.003053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30%</th>\n",
       "      <td>0.010149</td>\n",
       "      <td>-0.000643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40%</th>\n",
       "      <td>0.010891</td>\n",
       "      <td>0.001895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.011941</td>\n",
       "      <td>0.004587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60%</th>\n",
       "      <td>0.013307</td>\n",
       "      <td>0.008741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70%</th>\n",
       "      <td>0.014596</td>\n",
       "      <td>0.011693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80%</th>\n",
       "      <td>0.018323</td>\n",
       "      <td>0.016012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90%</th>\n",
       "      <td>0.024418</td>\n",
       "      <td>0.020960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95%</th>\n",
       "      <td>0.030575</td>\n",
       "      <td>0.025746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99%</th>\n",
       "      <td>0.036371</td>\n",
       "      <td>0.032915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.052060</td>\n",
       "      <td>0.049213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        TRAIN_All    TEST_All\n",
       "count  176.000000  173.000000\n",
       "mean     0.009022    0.005629\n",
       "std      0.014403    0.012550\n",
       "min     -0.021032   -0.040173\n",
       "1%      -0.017761   -0.022310\n",
       "5%      -0.014766   -0.013395\n",
       "10%     -0.013404   -0.010332\n",
       "20%     -0.010516   -0.003053\n",
       "30%      0.010149   -0.000643\n",
       "40%      0.010891    0.001895\n",
       "50%      0.011941    0.004587\n",
       "60%      0.013307    0.008741\n",
       "70%      0.014596    0.011693\n",
       "80%      0.018323    0.016012\n",
       "90%      0.024418    0.020960\n",
       "95%      0.030575    0.025746\n",
       "99%      0.036371    0.032915\n",
       "max      0.052060    0.049213"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_list          = [.01, .05, .1, .2, .3, .4, .6, .7, .8, .9, .95, .99]\n",
    "corr_train_all  = train_X_FE_50Corr.apply(lambda x: x.corr(train_y)).to_frame().describe(percentiles=p_list)\n",
    "corr_test_all   = test_X_FE_50Corr.apply(lambda x: x.corr(test_y)).to_frame().describe(percentiles=p_list)\n",
    "\n",
    "corr_all         = pd.concat([corr_train_all, corr_test_all], axis=1)\n",
    "corr_all.columns = ['TRAIN_All', 'TEST_All']\n",
    "print('\\n' + '*'*50 + '\\nCorrelation Summary: TRAIN vs TEST\\n' + '*'*50)\n",
    "print('abs(correlation) among Features <= 0.50')\n",
    "corr_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Feature-Engineered Data to Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-Use Transformed/Selected Featuers Again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "TRAIN vs TEST Datasets\n",
      "**************************************************\n",
      "The Shape of TRAIN Data: (106176, 598)\n",
      "The Shape of TEST Data:  (102062, 598)\n",
      "\n",
      "**************************************************\n",
      "Overall Churn Rate\n",
      "**************************************************\n",
      "TRAIN:  0.0252\n",
      "TEST:   0.0332 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# abs(correlation ) among Features <= 0.75\n",
    "df_train = pd.concat([df_train_FE_NUM, train_X_FE], axis=1)\n",
    "df_test  = pd.concat([df_test_FE_NUM, test_X_FE], axis=1)\n",
    "\n",
    "# TRAIN\n",
    "train_X  = df_train.drop('status', axis=1).copy()\n",
    "train_y  = df_train['status']\n",
    "\n",
    "# TEST\n",
    "test_X   = df_test.drop('status', axis=1).copy()\n",
    "test_y   = df_test['status']\n",
    "\n",
    "# Sample Size\n",
    "print('*'*50 + '\\nTRAIN vs TEST Datasets\\n' + '*'*50)\n",
    "print('The Shape of TRAIN Data: ' + str(df_train.shape))\n",
    "print('The Shape of TEST Data:  ' + str(df_test.shape))\n",
    "\n",
    "## Churn Rate by Sample Type\n",
    "print('\\n' + '*'*50 + '\\nOverall Churn Rate\\n' + '*'*50)\n",
    "print('TRAIN: ', df_train.status.value_counts(normalize=True)[1].round(4))\n",
    "print('TEST:  ', df_test.status.value_counts(normalize=True)[1].round(4), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### abs(correlation ) among Features <= 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Remove_IrrelevantCollinearFeatures' requires target y.\n",
      "\n",
      "****************************************************************************************************\n",
      "Total NUMERICAL Features: 597 features examined\n",
      "Irrelevant Features     : 0 features dropped due to abs(correlation ) <= 0.01 with target\n",
      "Collinear Features      : 12 features dropped due to abs(correlation ) > 0.75 with other features\n",
      "****************************************************************************************************\n",
      "\n",
      "**************************************************\n",
      "Before vs After Feature Engineering (FE)\n",
      "**************************************************\n",
      "TRAIN: Before FE:(106176, 597)\n",
      "TRAIN: After FE: (106176, 585)\n",
      "TEST:  After FE: (102062, 585)\n"
     ]
    }
   ],
   "source": [
    "# corr_target   = 0.01  # To remove features with low correlation with target: abs(Corr w. Target) < 0.01\n",
    "# corr_features = 0.75  # To remove features with high correlation with other features: abs(Corr) > 0.75\n",
    "\n",
    "# (1) Instantiate\n",
    "RICF = FE.Remove_IrrelevantCollinearFeatures(corr_threshold_target=0.01, corr_threshold_features=0.75)\n",
    "\n",
    "# (2) Fit\n",
    "RICF.fit(train_X, train_y)\n",
    "\n",
    "# (3) Transform\n",
    "train_X_FE = RICF.transform(train_X)\n",
    "test_X_FE  = RICF.transform(test_X)\n",
    "\n",
    "# Feature Dimension\n",
    "print('\\n' + '*'*50 + '\\nBefore vs After Feature Engineering (FE)' + '\\n' + '*'*50)\n",
    "print('TRAIN: Before FE:' + str(train_X.shape))\n",
    "print('TRAIN: After FE: ' + str(train_X_FE.shape))\n",
    "print('TEST:  After FE: ' + str(test_X_FE.shape))\n",
    "\n",
    "# abs(correlation) among Features <= 0.50\n",
    "df_train_FE = pd.concat([train_y, train_X_FE], axis=1)\n",
    "df_test_FE  = pd.concat([test_y, test_X_FE], axis=1)\n",
    "\n",
    "df_train_FE.to_pickle('data_processed/FIOS_ONT_G1_4_train_FE_75Corr.pkl')\n",
    "df_test_FE.to_pickle('data_processed/FIOS_ONT_G1_4_train_FE_75Corr.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### abs(correlation ) among Features <= 0.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Remove_IrrelevantCollinearFeatures' requires target y.\n",
      "\n",
      "****************************************************************************************************\n",
      "Total NUMERICAL Features: 597 features examined\n",
      "Irrelevant Features     : 0 features dropped due to abs(correlation ) <= 0.01 with target\n",
      "Collinear Features      : 326 features dropped due to abs(correlation ) > 0.5 with other features\n",
      "****************************************************************************************************\n",
      "\n",
      "**************************************************\n",
      "Before vs After Feature Engineering (FE)\n",
      "**************************************************\n",
      "TRAIN: Before FE:(106176, 597)\n",
      "TRAIN: After FE: (106176, 271)\n",
      "TEST:  After FE: (102062, 271)\n"
     ]
    }
   ],
   "source": [
    "# corr_target   = 0.01  # To remove features with low correlation with target: abs(Corr w. Target) < 0.01\n",
    "# corr_features = 0.50  # To remove features with high correlation with other features: abs(Corr) > 0.50\n",
    "\n",
    "# (1) Instantiate\n",
    "RICF = FE.Remove_IrrelevantCollinearFeatures(corr_threshold_target=0.01, corr_threshold_features=0.50)\n",
    "\n",
    "# (2) Fit\n",
    "RICF.fit(train_X, train_y)\n",
    "\n",
    "# (3) Transform\n",
    "train_X_FE = RICF.transform(train_X)\n",
    "test_X_FE  = RICF.transform(test_X)\n",
    "\n",
    "# Feature Dimension\n",
    "print('\\n' + '*'*50 + '\\nBefore vs After Feature Engineering (FE)' + '\\n' + '*'*50)\n",
    "print('TRAIN: Before FE:' + str(train_X.shape))\n",
    "print('TRAIN: After FE: ' + str(train_X_FE.shape))\n",
    "print('TEST:  After FE: ' + str(test_X_FE.shape))\n",
    "\n",
    "# abs(correlation) among Features <= 0.50\n",
    "df_train_FE = pd.concat([train_y, train_X_FE], axis=1)\n",
    "df_test_FE  = pd.concat([test_y, test_X_FE], axis=1)\n",
    "\n",
    "df_train_FE.to_pickle('data_processed/FIOS_ONT_G1_4_train_FE_50Corr.pkl')\n",
    "df_test_FE.to_pickle('data_processed/FIOS_ONT_G1_4_train_FE_50Corr.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### abs(correlation ) among Features <= 0.30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Remove_IrrelevantCollinearFeatures' requires target y.\n",
      "\n",
      "****************************************************************************************************\n",
      "Total NUMERICAL Features: 597 features examined\n",
      "Irrelevant Features     : 0 features dropped due to abs(correlation ) <= 0.01 with target\n",
      "Collinear Features      : 490 features dropped due to abs(correlation ) > 0.3 with other features\n",
      "****************************************************************************************************\n",
      "\n",
      "**************************************************\n",
      "Before vs After Feature Engineering (FE)\n",
      "**************************************************\n",
      "TRAIN: Before FE:(106176, 597)\n",
      "TRAIN: After FE: (106176, 107)\n",
      "TEST:  After FE: (102062, 107)\n"
     ]
    }
   ],
   "source": [
    "# corr_target   = 0.01  # To remove features with low correlation with target: abs(Corr w. Target) < 0.01\n",
    "# corr_features = 0.50  # To remove features with high correlation with other features: abs(Corr) > 0.30\n",
    "\n",
    "# (1) Instantiate\n",
    "RICF = FE.Remove_IrrelevantCollinearFeatures(corr_threshold_target=0.01, corr_threshold_features=0.30)\n",
    "\n",
    "# (2) Fit\n",
    "RICF.fit(train_X, train_y)\n",
    "\n",
    "# (3) Transform\n",
    "train_X_FE = RICF.transform(train_X)\n",
    "test_X_FE  = RICF.transform(test_X)\n",
    "\n",
    "# Feature Dimension\n",
    "print('\\n' + '*'*50 + '\\nBefore vs After Feature Engineering (FE)' + '\\n' + '*'*50)\n",
    "print('TRAIN: Before FE:' + str(train_X.shape))\n",
    "print('TRAIN: After FE: ' + str(train_X_FE.shape))\n",
    "print('TEST:  After FE: ' + str(test_X_FE.shape))\n",
    "\n",
    "# abs(correlation) among Features <= 0.50\n",
    "df_train_FE = pd.concat([train_y, train_X_FE], axis=1)\n",
    "df_test_FE  = pd.concat([test_y, test_X_FE], axis=1)\n",
    "\n",
    "df_train_FE.to_pickle('data_processed/FIOS_ONT_G1_4_train_FE_30Corr.pkl')\n",
    "df_test_FE.to_pickle('data_processed/FIOS_ONT_G1_4_train_FE_30Corr.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gsutil cp SEG*.pkl gs://alticeusa-am_tkim/pre-processed_data/new"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
