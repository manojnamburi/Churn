{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create TRAIN/TEST Datasets for Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Load Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Default Packages\n",
    "import pandas as pd\n",
    "import pandas_gbq as gbq\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "# import os\n",
    "import string\n",
    "# import matplotlib.cm as cm\n",
    "# from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "### Pipeline Packages\n",
    "from sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "### Custom Transformers\n",
    "import CLUSTER_FeatureEngineering as FE                        # Include 'competitive_area' as a feature\n",
    "from Class_GeneralUtilitiesNonTF import GeneralUtilitiesNonTF  # For BQ-Storage-Python Connection\n",
    "\n",
    "\n",
    "### Dictionaries for Data Type and Missing Value Imputation\n",
    "from attribute_dictionary import attribute_dict\n",
    "from imputation_dictionary import attribute_imputer_dict\n",
    "\n",
    "\n",
    "### Remove DataConversionWarning\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import TRAIN/TEST Datasets from BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing TRAIN/TEST Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datetime\n",
    "# import psycopg2\n",
    "# print('*'*25 + \"\\nProgram Start Time:\")\n",
    "# print(datetime.datetime.strftime(datetime.datetime.now(), '%Y-%m-%d %H:%M:%S') + '\\n'+'*'*25)\n",
    "\n",
    "# Do Not Change the Below Information Unless Use Others!\n",
    "bucket     = 'alticeusa-am'\n",
    "project_id = 'alticeusa-am'\n",
    "dataset    = 'poc'\n",
    "auth_file  = 'alticeusa-am-b639e404289b.json'\n",
    "\n",
    "#Instantiate the util obj that will be used to interact with BQ\n",
    "util_obj   = GeneralUtilitiesNonTF(project_id  = project_id, \n",
    "                                   dataset     = dataset,\n",
    "                                   bucket_name = bucket,\n",
    "                                   json_path   = auth_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temp table to be created: temp_gcs_05_24_19_14_49_43\n",
      "**********\n",
      "Query executed: \n",
      " SELECT * FROM poc.TK_SEG1_TRAIN_RAW; \n",
      "Temp table temp_gcs_05_24_19_14_49_43 created\n",
      "**********\n",
      "Info query: SELECT table_name,column_name,data_type,is_nullable FROM poc.INFORMATION_SCHEMA.COLUMNS WHERE table_name='temp_gcs_05_24_19_14_49_43'\n",
      "Extraction to the tmp_folder_05_24_19_14_49_43 complete\n",
      "Prefix: tmp_folder_05_24_19_14_49_43/tmp_file_05_24_19_14_49_43\n",
      "Pulling file: tmp_folder_05_24_19_14_49_43/tmp_file_05_24_19_14_49_43-000000000000.csv.gzip\n",
      "Pulling file: tmp_folder_05_24_19_14_49_43/tmp_file_05_24_19_14_49_43-000000000001.csv.gzip\n",
      "Deleted the temp folder tmp_folder_05_24_19_14_49_43\n",
      "\n",
      " ***************************************************************** \n",
      " TRAIN data is SUCCESSFULLY imported! Number of records = 99608.\n",
      " ***************************************************************** \n",
      "\n",
      "Temp table to be created: temp_gcs_05_24_19_14_51_58\n",
      "**********\n",
      "Query executed: \n",
      " SELECT * FROM poc.TK_SEG1_TEST_RAW; \n",
      "Temp table temp_gcs_05_24_19_14_51_58 created\n",
      "**********\n",
      "Info query: SELECT table_name,column_name,data_type,is_nullable FROM poc.INFORMATION_SCHEMA.COLUMNS WHERE table_name='temp_gcs_05_24_19_14_51_58'\n",
      "Extraction to the tmp_folder_05_24_19_14_51_58 complete\n",
      "Prefix: tmp_folder_05_24_19_14_51_58/tmp_file_05_24_19_14_51_58\n",
      "Pulling file: tmp_folder_05_24_19_14_51_58/tmp_file_05_24_19_14_51_58-000000000000.csv.gzip\n",
      "Pulling file: tmp_folder_05_24_19_14_51_58/tmp_file_05_24_19_14_51_58-000000000001.csv.gzip\n",
      "Deleted the temp folder tmp_folder_05_24_19_14_51_58\n",
      "\n",
      " ***************************************************************** \n",
      " TEST data is SUCCESSFULLY imported! Number of records = 94993.\n",
      " ***************************************************************** \n",
      "\n",
      "CPU times: user 35.3 s, sys: 3.6 s, total: 38.9 s\n",
      "Wall time: 3min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# TRAIN Dataset\n",
    "sql_data = ''' SELECT * FROM poc.TK_SEG1_TRAIN_RAW; ''' \n",
    "df_train = util_obj.read_gbq(sql_query = sql_data)\n",
    "print('\\n', '*'*65, '\\n', f\"TRAIN data is SUCCESSFULLY imported! Number of records = {str(len(df_train))}.\" \\\n",
    "      + '\\n', '*'*65, '\\n')\n",
    "               \n",
    "# TEST Dataset\n",
    "sql_data = ''' SELECT * FROM poc.TK_SEG1_TEST_RAW; ''' \n",
    "df_test  = util_obj.read_gbq(sql_query = sql_data)\n",
    "print('\\n', '*'*65, '\\n', f\"TEST data is SUCCESSFULLY imported! Number of records = {str(len(df_test))}.\" \\\n",
    "      + '\\n', '*'*65, '\\n')\n",
    "\n",
    "# CPU times: user 35.7 s, sys: 3.8 s, total: 39.5 s\n",
    "# Wall time: 4min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "TRAIN vs TEST Datasets\n",
      "**************************************************\n",
      "Competitive Area:  ['Fios Competitive Area' 'Fios ONT Competitive Area'\n",
      " 'Pre-Fios Competitive Area']\n",
      "The Shape of TRAIN Data: (99608, 1041)\n",
      "The Shape of TEST Data:  (94993, 1041)\n",
      "\n",
      "**************************************************\n",
      "Overall Churn Rate\n",
      "**************************************************\n",
      "TRAIN:  0.038\n",
      "TEST:   0.036 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use 'chc_id' as index, and sort by index.\n",
    "df_train.set_index('chc_id', inplace=True)\n",
    "df_test.set_index('chc_id', inplace=True)\n",
    "\n",
    "df_train = df_train.sort_index()\n",
    "df_test  = df_test.sort_index()\n",
    "\n",
    "# TRAIN\n",
    "train_X  = df_train.drop('status', axis=1).copy()\n",
    "train_y  = df_train['status']\n",
    "\n",
    "# TEST\n",
    "test_X   = df_test.drop('status', axis=1).copy()\n",
    "test_y   = df_test['status']\n",
    "\n",
    "# Sample Size\n",
    "print('*'*50 + '\\nTRAIN vs TEST Datasets\\n' + '*'*50)\n",
    "print('Competitive Area: ', df_train.competitive_area.unique())\n",
    "print('The Shape of TRAIN Data: ' + str(df_train.shape))\n",
    "print('The Shape of TEST Data:  ' + str(df_test.shape))\n",
    "\n",
    "## Churn Rate by Sample Type\n",
    "print('\\n' + '*'*50 + '\\nOverall Churn Rate\\n' + '*'*50)\n",
    "print('TRAIN: ', df_train.status.value_counts(normalize=True)[1].round(3))\n",
    "print('TEST:  ', df_test.status.value_counts(normalize=True)[1].round(3), '\\n')\n",
    "\n",
    "# print(train_X.index)\n",
    "# print(train_y.index)\n",
    "# print(test_X.index)\n",
    "# print(test_y.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-Process TRAIN/TEST Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Pre-Processing: Use_DefaultDataType\n",
      "**************************************************\n",
      "- It will convert data types into default ones.\n",
      "\n",
      "**************************************************\n",
      "Pre-Processing: Remove_MissingFeatures\n",
      "**************************************************\n",
      "- It will remove features with a high missing pct.\n",
      "\n",
      "**************************************************\n",
      "Pre-Processing: Remove_ConstantFeatures\n",
      "**************************************************\n",
      "- It will remove features with 1 unique value(s).\n",
      "\n",
      "**************************************************\n",
      "Pre-Processing: Remove_CorrelatedFeatures\n",
      "**************************************************\n",
      "- It will work on Numerical Features Only, doing nothing on Categorical Features.\n",
      "- It may take 10+ minutes. Be patient!\n",
      "\n",
      "**************************************************\n",
      "Pre-Processing: Remove_DuplicateFeatures\n",
      "**************************************************\n",
      "- It may take 10+ minutes. Be patient!\n",
      "\n",
      "**************************************************\n",
      "Pre-Processing: Use_DefaultImputere\n",
      "**************************************************\n",
      "- It will append default imputation values to missings.\n",
      "\n",
      "**************************************************\n",
      "Pre-Processing: Remove_ConstantFeatures\n",
      "**************************************************\n",
      "- It will remove features with 1 unique value(s).\n",
      "\n",
      "**************************************************\n",
      "Pre-Processing: Remove_CorrelatedFeatures\n",
      "**************************************************\n",
      "- It will work on Numerical Features Only, doing nothing on Categorical Features.\n",
      "- It may take 10+ minutes. Be patient!\n",
      "\n",
      "25 features with greater than 99.0% missing values\n",
      "26 features with 1 or fewer unique value(s)\n",
      "43 features with abs(correlation ) > 0.99 with other features\n",
      "4 features with duplicate columns\n",
      "17 features with 1 or fewer unique value(s)\n",
      "124 features with abs(correlation ) > 0.9 with other features\n",
      "\n",
      "**************************************************\n",
      "Before vs After Transformation\n",
      "**************************************************\n",
      "TRAIN: Before Transformation:(99608, 1040)\n",
      "TRAIN: After Transformation: (99608, 787)\n",
      "TEST:  After Transformation: (94993, 787)\n",
      "CPU times: user 29min 59s, sys: 10min 53s, total: 40min 53s\n",
      "Wall time: 41min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# (1) Make a Pipeline and Instantiate\n",
    "Pipe_PP = Pipeline([\n",
    "                    ('DataType', FE.Use_DefaultDataType(default_dtypes=attribute_dict)),\n",
    "                    ('Missing', FE.Remove_MissingFeatures(missing_threshold=0.99)), \n",
    "                    ('Constant1', FE.Remove_ConstantFeatures(unique_threshold=1, missing_threshold=0.00)), \n",
    "                    ('Correlated1', FE.Remove_CorrelatedFeatures(correlation_threshold=0.99)), \n",
    "                    ('Duplicate', FE.Remove_DuplicateFeatures()),\n",
    "                    ('Imputer', FE.Use_DefaultImputer(default_imputers=attribute_imputer_dict, default_dtypes=attribute_dict)),\n",
    "                    ('Constant2', FE.Remove_ConstantFeatures(unique_threshold=1, missing_threshold=0.00)), \n",
    "                    ('Correlated2', FE.Remove_CorrelatedFeatures(correlation_threshold=0.90))\n",
    "                  ])\n",
    "\n",
    "# 'Constant2' is added to handle (1) unique value = 0 and (2) default imputation value = 0.\n",
    "# 'Correlated2' is added to further remove correlated features after impuation.\n",
    "\n",
    "\n",
    "# (2) fit()\n",
    "Pipe_PP.fit(train_X, train_y)\n",
    "\n",
    "\n",
    "# (3) transform()\n",
    "train_X_PP = Pipe_PP.transform(train_X)\n",
    "test_X_PP  = Pipe_PP.transform(test_X)\n",
    "\n",
    "# Feature Dimension\n",
    "print('\\n' + '*'*50 + '\\nBefore vs After Transformation\\n' + '*'*50)\n",
    "print('TRAIN: Before Transformation:' + str(train_X.shape))\n",
    "print('TRAIN: After Transformation: ' + str(train_X_PP.shape))\n",
    "print('TEST:  After Transformation: ' + str(test_X_PP.shape))\n",
    "\n",
    "# 25 features with greater than 99.0% missing values\n",
    "# 26 features with 1 or fewer unique value(s)\n",
    "# 43 features with abs(correlation ) > 0.99 with other features\n",
    "# 4 features with duplicate columns\n",
    "# 17 features with 1 or fewer unique value(s)\n",
    "# 124 features with abs(correlation ) > 0.9 with other features\n",
    "\n",
    "# **************************************************\n",
    "# Before vs After Transformation\n",
    "# **************************************************\n",
    "# TRAIN: Before Transformation:(99608, 1040)\n",
    "# TRAIN: After Transformation: (99608, 787)\n",
    "# TEST:  After Transformation: (94993, 787)\n",
    "        \n",
    "# CPU times: user 26min 18s, sys: 9min 10s, total: 35min 29s\n",
    "# Wall time: 35min 29s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Pre-Processing: Use_DefaultImputere\n",
      "**************************************************\n",
      "- It will append default imputation values to missings.\n",
      "\n",
      "\n",
      "**************************************************\n",
      "Before vs After Transformation\n",
      "**************************************************\n",
      "TRAIN: Before Transformation:(99608, 1040)\n",
      "TRAIN: After Transformation: (99608, 16)\n",
      "TEST:  After Transformation: (94993, 16)\n",
      "\n",
      "**************************************************\n",
      "Newly Created Features\n",
      "**************************************************\n",
      " ['grp_tenure_3m', 'grp_tenure_1m', 'grp_tenure_6m', 'grp_payment_method', 'grp_payment_25dollar', 'grp_payment_10dollar', 'grp_payment_change_5dollar', 'grp_payment_change_10dollar', 'grp_payment_change_2pct', 'grp_payment_change_5pct', 'ratio_payment_income', 'grp_payment_income', 'grp_call_csc', 'grp_call_bill', 'grp_call_csr', 'grp_call_tsr']\n",
      "CPU times: user 8min 55s, sys: 6min 48s, total: 15min 43s\n",
      "Wall time: 15min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# (1) Make a Pipeline and Instantiate\n",
    "Pipe_NF = Pipeline([\n",
    "                    ('Imputer', FE.Use_DefaultImputer(default_imputers=attribute_imputer_dict, default_dtypes=attribute_dict)),\n",
    "                    ('NewFeatures', FE.FeatureMaker())\n",
    "                  ])\n",
    "\n",
    "# 'Imputer' is added to handle missing values\n",
    "# 'NewFeature' is added to create new features\n",
    "\n",
    "\n",
    "# (2) fit()\n",
    "Pipe_NF.fit(train_X, train_y)\n",
    "\n",
    "\n",
    "# (3) transform()\n",
    "train_X_NF = Pipe_NF.transform(train_X)\n",
    "test_X_NF  = Pipe_NF.transform(test_X)\n",
    "\n",
    "# Feature Dimension\n",
    "print('\\n' + '*'*50 + '\\nBefore vs After Transformation\\n' + '*'*50)\n",
    "print('TRAIN: Before Transformation:' + str(train_X.shape))\n",
    "print('TRAIN: After Transformation: ' + str(train_X_NF.shape))\n",
    "print('TEST:  After Transformation: ' + str(test_X_NF.shape))\n",
    "print('\\n' + '*'*50 + '\\nNewly Created Features\\n' + '*'*50 + '\\n', \n",
    "      Pipe_NF.named_steps['NewFeatures'].features_new_)\n",
    "\n",
    "# CPU times: user 9min, sys: 6min 19s, total: 15min 20s\n",
    "# Wall time: 15min 16s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Processed and New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99608, 804)\n",
      "(94993, 804)\n"
     ]
    }
   ],
   "source": [
    "# Create Datasets that Consist of Pre-processed and New Features.\n",
    "df_train_NF_PP = train_y.to_frame().\\\n",
    "                 merge(train_X_NF, how='inner', left_index=True, right_index=True).\\\n",
    "                 merge(train_X_PP, how='inner', left_index=True, right_index=True)\n",
    "\n",
    "df_test_NF_PP  = test_y.to_frame().\\\n",
    "                 merge(test_X_NF, how='inner', left_index=True, right_index=True).\\\n",
    "                 merge(test_X_PP, how='inner', left_index=True, right_index=True)\n",
    "\n",
    "# # Save Data for Feature Engineering\n",
    "# # Pre-processed data with new features\n",
    "df_train_NF_PP.to_pickle('data_new/SEG1_train_PP.pkl')\n",
    "df_test_NF_PP.to_pickle('data_new/SEG1_test_PP.pkl')\n",
    "\n",
    "print(df_train_NF_PP.shape)\n",
    "print(df_test_NF_PP.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FE for NUMERICAL Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99608, 803)\n",
      "(99608, 803)\n"
     ]
    }
   ],
   "source": [
    "# Use Pre-Processed Data as new TRAIN/TEST Datasets\n",
    "train_X = train_X_NF.merge(train_X_PP, how='inner', left_index=True, right_index=True)\n",
    "test_X  = test_X_NF.merge(test_X_PP, how='inner', left_index=True, right_index=True)\n",
    "print(train_X.shape)\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**************************************************\n",
      "Before vs After Feature Engineering (FE)\n",
      "**************************************************\n",
      "TRAIN: Before FE:(99608, 803)\n",
      "TRAIN: After FE: (99608, 4980)\n",
      "TEST:  After FE: (94993, 4980)\n",
      "CPU times: user 26.2 s, sys: 15.3 s, total: 41.5 s\n",
      "Wall time: 41.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# (1) Make a Pipeline in Parallel/Sequence and Instantiate \n",
    "# Custom Transformers in Parallel for NUMERICAL Features\n",
    "Pipe_FU          =  FE.FeatureUnion_DF([\n",
    "                    ('Original', FE.PassTransformer(prefix='Original')),\n",
    "                    ('Standard', FE.StandardScaler_DF(prefix='Standard')),\n",
    "                    ('Robust', FE.RobustScaler_DF(prefix='Robust', quantile_range=(5.0, 95.0))),\n",
    "                    ('Quantile', FE.QuantileTransformer_DF(prefix='Quantile', n_quantiles=100, random_state=0)),\n",
    "                    ('Binary', FE.Binarizer_DF(prefix='Binary', threshold=0)),\n",
    "                    ('MinMax', FE.MinMaxScaler_DF(prefix='MinMax', feature_range=(0, 1))),\n",
    "                    ('MaxAbs', FE.MaxAbsScaler_DF(prefix='MaxAbs')),\n",
    "                    ('Norm', FE.Normalizer_DF(prefix='Norm', norm='l1')),\n",
    "                    ('KBin', FE.KBinsDiscretizer_DF(prefix='KBin', n_bins=10, encode='ordinal')),\n",
    "                    ('Log1p', FE.Log1pTransformer(prefix='Log1p')),\n",
    "                    ('Sqrt', FE.SqrtTransformer(prefix='Sqrt')),\n",
    "                    ('Reciprocal', FE.ReciprocalTransformer(prefix='Reciprocal'))\n",
    "                    ])\n",
    "\n",
    "# Custom Transformers in Sequence for NUMERICAL Features\n",
    "NUM_Pipe          = Pipeline([\n",
    "                    ('Selector', FE.FeatureSelector_NUM()),\n",
    "                    ('FU_Pipe', Pipe_FU)\n",
    "                    ])\n",
    "\n",
    "\n",
    "# (2) fit()\n",
    "NUM_Pipe.fit(train_X, train_y)\n",
    "\n",
    "\n",
    "# (3) transform()\n",
    "train_X_FE = NUM_Pipe.transform(train_X)\n",
    "test_X_FE  = NUM_Pipe.transform(test_X)\n",
    "\n",
    "# Feature Dimension\n",
    "print('\\n' + '*'*50 + '\\nBefore vs After Feature Engineering (FE)\\n' + '*'*50)\n",
    "print('TRAIN: Before FE:' + str(train_X.shape))\n",
    "print('TRAIN: After FE: ' + str(train_X_FE.shape))\n",
    "print('TEST:  After FE: ' + str(test_X_FE.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**************************************************\n",
      "Correlation Summary: TRAIN vs TEST\n",
      "**************************************************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRAIN_Original</th>\n",
       "      <th>TRAIN_All</th>\n",
       "      <th>TEST_Original</th>\n",
       "      <th>TEST_All</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>415.000000</td>\n",
       "      <td>4733.000000</td>\n",
       "      <td>413.000000</td>\n",
       "      <td>4716.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.000560</td>\n",
       "      <td>-0.000586</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.001360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.007395</td>\n",
       "      <td>0.007191</td>\n",
       "      <td>0.007841</td>\n",
       "      <td>0.007825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.032097</td>\n",
       "      <td>-0.034218</td>\n",
       "      <td>-0.029401</td>\n",
       "      <td>-0.035188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1%</th>\n",
       "      <td>-0.021667</td>\n",
       "      <td>-0.021400</td>\n",
       "      <td>-0.019741</td>\n",
       "      <td>-0.016083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5%</th>\n",
       "      <td>-0.011885</td>\n",
       "      <td>-0.011783</td>\n",
       "      <td>-0.007485</td>\n",
       "      <td>-0.008029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10%</th>\n",
       "      <td>-0.008424</td>\n",
       "      <td>-0.008049</td>\n",
       "      <td>-0.006103</td>\n",
       "      <td>-0.006029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20%</th>\n",
       "      <td>-0.004463</td>\n",
       "      <td>-0.004479</td>\n",
       "      <td>-0.003429</td>\n",
       "      <td>-0.003604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30%</th>\n",
       "      <td>-0.003241</td>\n",
       "      <td>-0.003061</td>\n",
       "      <td>-0.001777</td>\n",
       "      <td>-0.002216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40%</th>\n",
       "      <td>-0.001967</td>\n",
       "      <td>-0.001983</td>\n",
       "      <td>-0.000833</td>\n",
       "      <td>-0.000998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.000839</td>\n",
       "      <td>-0.000942</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60%</th>\n",
       "      <td>0.000188</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.001630</td>\n",
       "      <td>0.001481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70%</th>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.001275</td>\n",
       "      <td>0.002955</td>\n",
       "      <td>0.002973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80%</th>\n",
       "      <td>0.003885</td>\n",
       "      <td>0.003434</td>\n",
       "      <td>0.006050</td>\n",
       "      <td>0.005769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90%</th>\n",
       "      <td>0.009079</td>\n",
       "      <td>0.008415</td>\n",
       "      <td>0.010952</td>\n",
       "      <td>0.010652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95%</th>\n",
       "      <td>0.012171</td>\n",
       "      <td>0.012520</td>\n",
       "      <td>0.014904</td>\n",
       "      <td>0.015551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99%</th>\n",
       "      <td>0.016357</td>\n",
       "      <td>0.018822</td>\n",
       "      <td>0.030045</td>\n",
       "      <td>0.030968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.032700</td>\n",
       "      <td>0.040406</td>\n",
       "      <td>0.037069</td>\n",
       "      <td>0.047027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       TRAIN_Original    TRAIN_All  TEST_Original     TEST_All\n",
       "count      415.000000  4733.000000     413.000000  4716.000000\n",
       "mean        -0.000560    -0.000586       0.001400     0.001360\n",
       "std          0.007395     0.007191       0.007841     0.007825\n",
       "min         -0.032097    -0.034218      -0.029401    -0.035188\n",
       "1%          -0.021667    -0.021400      -0.019741    -0.016083\n",
       "5%          -0.011885    -0.011783      -0.007485    -0.008029\n",
       "10%         -0.008424    -0.008049      -0.006103    -0.006029\n",
       "20%         -0.004463    -0.004479      -0.003429    -0.003604\n",
       "30%         -0.003241    -0.003061      -0.001777    -0.002216\n",
       "40%         -0.001967    -0.001983      -0.000833    -0.000998\n",
       "50%         -0.000839    -0.000942       0.000249     0.000100\n",
       "60%          0.000188    -0.000030       0.001630     0.001481\n",
       "70%          0.001541     0.001275       0.002955     0.002973\n",
       "80%          0.003885     0.003434       0.006050     0.005769\n",
       "90%          0.009079     0.008415       0.010952     0.010652\n",
       "95%          0.012171     0.012520       0.014904     0.015551\n",
       "99%          0.016357     0.018822       0.030045     0.030968\n",
       "max          0.032700     0.040406       0.037069     0.047027"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_list          = [.01, .05, .1, .2, .3, .4, .6, .7, .8, .9, .95, .99]\n",
    "flag_NUM        = train_X.select_dtypes(exclude=[object, 'category']).columns.tolist()\n",
    "corr_train      = train_X[flag_NUM].apply(lambda x: x.corr(train_y)).to_frame().describe(percentiles=p_list)\n",
    "corr_train_all  = train_X_FE.apply(lambda x: x.corr(train_y)).to_frame().describe(percentiles=p_list)\n",
    "\n",
    "corr_test       = test_X[flag_NUM].apply(lambda x: x.corr(test_y)).to_frame().describe(percentiles=p_list)\n",
    "corr_test_all   = test_X_FE.apply(lambda x: x.corr(test_y)).to_frame().describe(percentiles=p_list)\n",
    "\n",
    "corr_all         = pd.concat([corr_train, corr_train_all, corr_test, corr_test_all], axis=1)\n",
    "corr_all.columns = ['TRAIN_Original', 'TRAIN_All', 'TEST_Original', 'TEST_All']\n",
    "print('\\n' + '*'*50 + '\\nCorrelation Summary: TRAIN vs TEST\\n' + '*'*50)\n",
    "corr_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_FE_NUM = train_y.to_frame().\\\n",
    "                 merge(train_X_FE, how='inner', left_index=True, right_index=True)\n",
    "\n",
    "df_test_FE_NUM  = test_y.to_frame().\\\n",
    "                  merge(test_X_FE, how='inner', left_index=True, right_index=True)\n",
    "\n",
    "df_train_FE_NUM.to_pickle('data_new/SEG1_train_FE_NUM.pkl')\n",
    "df_test_FE_NUM.to_pickle('data_new/SEG1_test_FE_NUM.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FE for CATEGORICAL Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ordinal' encoding requires target y.\n",
      "'y_mean' encoding requires target y.\n",
      "'y_log_ratio' encoding requires target y.\n",
      "'y_ratio' encoding requires target y.\n",
      "'FeatureAggregator' requires target y.\n",
      "\n",
      "**************************************************\n",
      "Before vs After Feature Engineering (FE)\n",
      "**************************************************\n",
      "TRAIN: Before FE:(99608, 803)\n",
      "TRAIN: After FE: (99608, 11396)\n",
      "TEST:  After FE: (94993, 11396)\n",
      "CPU times: user 39min 23s, sys: 23min 30s, total: 1h 2min 54s\n",
      "Wall time: 1h 2min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# (1) Make a Pipeline in Parallel/Sequence and Instantiate \n",
    "# List of Features Used as Parameters\n",
    "fe_1st           = ['grp_tenure_3m', 'grp_payment_method', \\\n",
    "                    'grp_payment_25dollar', 'grp_payment_change_10dollar', 'grp_payment_change_5pct', \\\n",
    "                    'grp_payment_income', 'grp_call_csc', 'grp_call_bill', \\\n",
    "                    'grp_call_csr', 'grp_call_tsr']\n",
    "fe_2nd           = fe_1st + ['income_demos', 'ethnic', 'age_demos', 'archetype']\n",
    "fe_group         = ['census', 'cleansed_city', 'cleansed_zipcode']\n",
    "\n",
    "# Custom Transformers in Parallel for CATEGORICAL Features\n",
    "Pipe_FU          =  FE.FeatureUnion_DF([\n",
    "                    ('OHE', FE.UniversalCategoryEncoder(encoding_method='ohe')),\n",
    "                    ('PCT', FE.UniversalCategoryEncoder(encoding_method='pct', prefix='PCT')),\n",
    "                    ('COUNT', FE.UniversalCategoryEncoder(encoding_method='count', prefix='COUNT')),\n",
    "                    ('ORDINAL', FE.UniversalCategoryEncoder(encoding_method='ordinal', prefix='ORDINAL')),\n",
    "                    ('Y_MEAN', FE.UniversalCategoryEncoder(encoding_method='y_mean', prefix='Y_MEAN')),\n",
    "                    ('Y_LOG_RATIO', FE.UniversalCategoryEncoder(encoding_method='y_log_ratio', prefix='Y_LOG_RATIO')),\n",
    "                    ('Y_RATIO', FE.UniversalCategoryEncoder(encoding_method='y_ratio', prefix='Y_RATIO')),\n",
    "                    ('Aggregation', FE.FeatureAggregator(features_grouping=fe_group, correlation_threshold=0.01))\n",
    "                    ])\n",
    "\n",
    "# Custom Transformers in Sequence for CATEGORICAL Features\n",
    "CAT_Pipe          = Pipeline([\n",
    "                    ('Interaction', FE.FeatureInteractionTransformer(features_1st=fe_1st, features_2nd=fe_2nd)),\n",
    "                    ('RareCategory', FE.RareCategoryEncoder(category_min_pct=0.01, category_max_count=30)),\n",
    "                    ('FU_Pipe', Pipe_FU)\n",
    "                    ])\n",
    "\n",
    "# (2) fit()\n",
    "CAT_Pipe.fit(train_X, train_y)\n",
    "\n",
    "\n",
    "# (3) transform()\n",
    "train_X_FE = CAT_Pipe.transform(train_X)\n",
    "test_X_FE  = CAT_Pipe.transform(test_X)\n",
    "\n",
    "# Feature Dimension\n",
    "print('\\n' + '*'*50 + '\\nBefore vs After Feature Engineering (FE)\\n' + '*'*50)\n",
    "print('TRAIN: Before FE:' + str(train_X.shape))\n",
    "print('TRAIN: After FE: ' + str(train_X_FE.shape))\n",
    "print('TEST:  After FE: ' + str(test_X_FE.shape))\n",
    "\n",
    "# **************************************************\n",
    "# Before vs After Feature Engineering (FE)\n",
    "# **************************************************\n",
    "# TRAIN: Before FE:(99608, 803)\n",
    "# TRAIN: After FE: (99608, 11396)\n",
    "# TEST:  After FE: (94993, 11396)\n",
    "\n",
    "# **********************************************************************\n",
    "# Correlation Summary: TRAIN vs TEST\n",
    "# **********************************************************************\n",
    "# CPU times: user 38min, sys: 22min 56s, total: 1h 57s\n",
    "# Wall time: 1h 57s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**********************************************************************\n",
      "Correlation Summary: TRAIN vs TEST\n",
      "**********************************************************************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRAIN_All</th>\n",
       "      <th>TEST_All</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11309.000000</td>\n",
       "      <td>11233.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.001260</td>\n",
       "      <td>-0.000473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.014213</td>\n",
       "      <td>0.016353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.028932</td>\n",
       "      <td>-0.051032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1%</th>\n",
       "      <td>-0.020498</td>\n",
       "      <td>-0.029553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5%</th>\n",
       "      <td>-0.017811</td>\n",
       "      <td>-0.024317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10%</th>\n",
       "      <td>-0.017302</td>\n",
       "      <td>-0.023719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20%</th>\n",
       "      <td>-0.012875</td>\n",
       "      <td>-0.015811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30%</th>\n",
       "      <td>-0.009599</td>\n",
       "      <td>-0.007325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40%</th>\n",
       "      <td>-0.002573</td>\n",
       "      <td>-0.003217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.001091</td>\n",
       "      <td>-0.000260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60%</th>\n",
       "      <td>0.003523</td>\n",
       "      <td>0.001996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70%</th>\n",
       "      <td>0.009464</td>\n",
       "      <td>0.005730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80%</th>\n",
       "      <td>0.013069</td>\n",
       "      <td>0.012495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90%</th>\n",
       "      <td>0.019458</td>\n",
       "      <td>0.020275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95%</th>\n",
       "      <td>0.025704</td>\n",
       "      <td>0.027312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99%</th>\n",
       "      <td>0.040832</td>\n",
       "      <td>0.045378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.060508</td>\n",
       "      <td>0.057036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          TRAIN_All      TEST_All\n",
       "count  11309.000000  11233.000000\n",
       "mean       0.001260     -0.000473\n",
       "std        0.014213      0.016353\n",
       "min       -0.028932     -0.051032\n",
       "1%        -0.020498     -0.029553\n",
       "5%        -0.017811     -0.024317\n",
       "10%       -0.017302     -0.023719\n",
       "20%       -0.012875     -0.015811\n",
       "30%       -0.009599     -0.007325\n",
       "40%       -0.002573     -0.003217\n",
       "50%        0.001091     -0.000260\n",
       "60%        0.003523      0.001996\n",
       "70%        0.009464      0.005730\n",
       "80%        0.013069      0.012495\n",
       "90%        0.019458      0.020275\n",
       "95%        0.025704      0.027312\n",
       "99%        0.040832      0.045378\n",
       "max        0.060508      0.057036"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_list          = [.01, .05, .1, .2, .3, .4, .6, .7, .8, .9, .95, .99]\n",
    "corr_train_all  = train_X_FE.apply(lambda x: x.corr(train_y)).to_frame().describe(percentiles=p_list)\n",
    "corr_test_all   = test_X_FE.apply(lambda x: x.corr(test_y)).to_frame().describe(percentiles=p_list)\n",
    "\n",
    "corr_all         = pd.concat([corr_train_all, corr_test_all], axis=1)\n",
    "corr_all.columns = ['TRAIN_All', 'TEST_All']\n",
    "print('\\n' + '*'*70 + '\\nCorrelation Summary: TRAIN vs TEST\\n' + '*'*70)\n",
    "corr_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_FE_CAT = train_y.to_frame().\\\n",
    "                  merge(train_X_FE, how='inner', left_index=True, right_index=True)\n",
    "\n",
    "df_test_FE_CAT  = test_y.to_frame().\\\n",
    "                  merge(test_X_FE, how='inner', left_index=True, right_index=True)\n",
    "\n",
    "# df_train_FE_CAT.to_pickle('data_new/SEG1_train_FE_CAT.pkl')\n",
    "# df_test_FE_CAT.to_pickle('data_new/SEG1_test_FE_CAT.pkl')\n",
    "train_X_FE.to_pickle('data_new/SEG1_train_FE_CAT.pkl')\n",
    "test_X_FE.to_pickle('data_new/SEG1_test_FE_CAT.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_FE.to_pickle('data_new/SEG1_train_FE_CAT.pkl')\n",
    "test_X_FE.to_pickle('data_new/SEG1_test_FE_CAT.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRP_CALL_BILL-age_demos_0 Call___Unknown</th>\n",
       "      <th>GRP_CALL_BILL-age_demos_0 Call___25-34</th>\n",
       "      <th>GRP_CALL_BILL-age_demos_0 Call___45-54</th>\n",
       "      <th>GRP_CALL_BILL-age_demos_all_other</th>\n",
       "      <th>GRP_CALL_BILL-age_demos_0 Call___35-44</th>\n",
       "      <th>GRP_CALL_BILL-age_demos_0 Call___55-64</th>\n",
       "      <th>GRP_CALL_BILL-age_demos_1 Calls___Unknown</th>\n",
       "      <th>GRP_CALL_BILL-age_demos_1 Calls___25-34</th>\n",
       "      <th>GRP_CALL_BILL-age_demos_0 Call___65-74</th>\n",
       "      <th>GRP_CALL_BILL-age_demos_1 Calls___45-54</th>\n",
       "      <th>...</th>\n",
       "      <th>CLEANSED_ZIPCODE__*__videopromo_rng_m3__*__e: 14-15</th>\n",
       "      <th>CLEANSED_ZIPCODE__*__videopromo_rng_m3__*__f: 16-21</th>\n",
       "      <th>CLEANSED_ZIPCODE__*__videopromo_rng_m3__*__g: 22+</th>\n",
       "      <th>CLEANSED_ZIPCODE__*__videopromo_rng_m3__*__x: None</th>\n",
       "      <th>CLEANSED_ZIPCODE__*__videopromo_rng_m4__*__f: 16-21</th>\n",
       "      <th>CLEANSED_ZIPCODE__*__videopromo_rng_m4__*__x: None</th>\n",
       "      <th>CLEANSED_ZIPCODE__*__voluntary_discos__*__max</th>\n",
       "      <th>CLEANSED_ZIPCODE__*__voluntary_discos__*__mean</th>\n",
       "      <th>CLEANSED_ZIPCODE__*__voluntary_discos__*__std</th>\n",
       "      <th>CLEANSED_ZIPCODE__*__voluntary_discos__*__sum</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chc_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>780100001104</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151111</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0.057778</td>\n",
       "      <td>0.148889</td>\n",
       "      <td>0.275556</td>\n",
       "      <td>0.151111</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.428889</td>\n",
       "      <td>0.74652</td>\n",
       "      <td>193.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780100002602</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151111</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0.057778</td>\n",
       "      <td>0.148889</td>\n",
       "      <td>0.275556</td>\n",
       "      <td>0.151111</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.428889</td>\n",
       "      <td>0.74652</td>\n",
       "      <td>193.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780100011404</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151111</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0.057778</td>\n",
       "      <td>0.148889</td>\n",
       "      <td>0.275556</td>\n",
       "      <td>0.151111</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.428889</td>\n",
       "      <td>0.74652</td>\n",
       "      <td>193.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780100017405</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054422</td>\n",
       "      <td>0.22449</td>\n",
       "      <td>0.068027</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.163265</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>0.57727</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780100029602</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151111</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0.057778</td>\n",
       "      <td>0.148889</td>\n",
       "      <td>0.275556</td>\n",
       "      <td>0.151111</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.428889</td>\n",
       "      <td>0.74652</td>\n",
       "      <td>193.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 11396 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              GRP_CALL_BILL-age_demos_0 Call___Unknown  \\\n",
       "chc_id                                                   \n",
       "780100001104                                         0   \n",
       "780100002602                                         0   \n",
       "780100011404                                         0   \n",
       "780100017405                                         0   \n",
       "780100029602                                         0   \n",
       "\n",
       "              GRP_CALL_BILL-age_demos_0 Call___25-34  \\\n",
       "chc_id                                                 \n",
       "780100001104                                       1   \n",
       "780100002602                                       0   \n",
       "780100011404                                       0   \n",
       "780100017405                                       0   \n",
       "780100029602                                       0   \n",
       "\n",
       "              GRP_CALL_BILL-age_demos_0 Call___45-54  \\\n",
       "chc_id                                                 \n",
       "780100001104                                       0   \n",
       "780100002602                                       0   \n",
       "780100011404                                       0   \n",
       "780100017405                                       0   \n",
       "780100029602                                       1   \n",
       "\n",
       "              GRP_CALL_BILL-age_demos_all_other  \\\n",
       "chc_id                                            \n",
       "780100001104                                  0   \n",
       "780100002602                                  0   \n",
       "780100011404                                  0   \n",
       "780100017405                                  0   \n",
       "780100029602                                  0   \n",
       "\n",
       "              GRP_CALL_BILL-age_demos_0 Call___35-44  \\\n",
       "chc_id                                                 \n",
       "780100001104                                       0   \n",
       "780100002602                                       0   \n",
       "780100011404                                       0   \n",
       "780100017405                                       0   \n",
       "780100029602                                       0   \n",
       "\n",
       "              GRP_CALL_BILL-age_demos_0 Call___55-64  \\\n",
       "chc_id                                                 \n",
       "780100001104                                       0   \n",
       "780100002602                                       1   \n",
       "780100011404                                       0   \n",
       "780100017405                                       0   \n",
       "780100029602                                       0   \n",
       "\n",
       "              GRP_CALL_BILL-age_demos_1 Calls___Unknown  \\\n",
       "chc_id                                                    \n",
       "780100001104                                          0   \n",
       "780100002602                                          0   \n",
       "780100011404                                          0   \n",
       "780100017405                                          0   \n",
       "780100029602                                          0   \n",
       "\n",
       "              GRP_CALL_BILL-age_demos_1 Calls___25-34  \\\n",
       "chc_id                                                  \n",
       "780100001104                                        0   \n",
       "780100002602                                        0   \n",
       "780100011404                                        0   \n",
       "780100017405                                        0   \n",
       "780100029602                                        0   \n",
       "\n",
       "              GRP_CALL_BILL-age_demos_0 Call___65-74  \\\n",
       "chc_id                                                 \n",
       "780100001104                                       0   \n",
       "780100002602                                       0   \n",
       "780100011404                                       0   \n",
       "780100017405                                       0   \n",
       "780100029602                                       0   \n",
       "\n",
       "              GRP_CALL_BILL-age_demos_1 Calls___45-54  \\\n",
       "chc_id                                                  \n",
       "780100001104                                        0   \n",
       "780100002602                                        0   \n",
       "780100011404                                        0   \n",
       "780100017405                                        1   \n",
       "780100029602                                        0   \n",
       "\n",
       "                                  ...                        \\\n",
       "chc_id                            ...                         \n",
       "780100001104                      ...                         \n",
       "780100002602                      ...                         \n",
       "780100011404                      ...                         \n",
       "780100017405                      ...                         \n",
       "780100029602                      ...                         \n",
       "\n",
       "              CLEANSED_ZIPCODE__*__videopromo_rng_m3__*__e: 14-15  \\\n",
       "chc_id                                                              \n",
       "780100001104                                           0.151111     \n",
       "780100002602                                           0.151111     \n",
       "780100011404                                           0.151111     \n",
       "780100017405                                           0.054422     \n",
       "780100029602                                           0.151111     \n",
       "\n",
       "              CLEANSED_ZIPCODE__*__videopromo_rng_m3__*__f: 16-21  \\\n",
       "chc_id                                                              \n",
       "780100001104                                            0.20000     \n",
       "780100002602                                            0.20000     \n",
       "780100011404                                            0.20000     \n",
       "780100017405                                            0.22449     \n",
       "780100029602                                            0.20000     \n",
       "\n",
       "              CLEANSED_ZIPCODE__*__videopromo_rng_m3__*__g: 22+  \\\n",
       "chc_id                                                            \n",
       "780100001104                                           0.057778   \n",
       "780100002602                                           0.057778   \n",
       "780100011404                                           0.057778   \n",
       "780100017405                                           0.068027   \n",
       "780100029602                                           0.057778   \n",
       "\n",
       "              CLEANSED_ZIPCODE__*__videopromo_rng_m3__*__x: None  \\\n",
       "chc_id                                                             \n",
       "780100001104                                           0.148889    \n",
       "780100002602                                           0.148889    \n",
       "780100011404                                           0.148889    \n",
       "780100017405                                           0.142857    \n",
       "780100029602                                           0.148889    \n",
       "\n",
       "              CLEANSED_ZIPCODE__*__videopromo_rng_m4__*__f: 16-21  \\\n",
       "chc_id                                                              \n",
       "780100001104                                           0.275556     \n",
       "780100002602                                           0.275556     \n",
       "780100011404                                           0.275556     \n",
       "780100017405                                           0.238095     \n",
       "780100029602                                           0.275556     \n",
       "\n",
       "              CLEANSED_ZIPCODE__*__videopromo_rng_m4__*__x: None  \\\n",
       "chc_id                                                             \n",
       "780100001104                                           0.151111    \n",
       "780100002602                                           0.151111    \n",
       "780100011404                                           0.151111    \n",
       "780100017405                                           0.163265    \n",
       "780100029602                                           0.151111    \n",
       "\n",
       "              CLEANSED_ZIPCODE__*__voluntary_discos__*__max  \\\n",
       "chc_id                                                        \n",
       "780100001104                                            3.0   \n",
       "780100002602                                            3.0   \n",
       "780100011404                                            3.0   \n",
       "780100017405                                            3.0   \n",
       "780100029602                                            3.0   \n",
       "\n",
       "              CLEANSED_ZIPCODE__*__voluntary_discos__*__mean  \\\n",
       "chc_id                                                         \n",
       "780100001104                                        0.428889   \n",
       "780100002602                                        0.428889   \n",
       "780100011404                                        0.428889   \n",
       "780100017405                                        0.265306   \n",
       "780100029602                                        0.428889   \n",
       "\n",
       "              CLEANSED_ZIPCODE__*__voluntary_discos__*__std  \\\n",
       "chc_id                                                        \n",
       "780100001104                                        0.74652   \n",
       "780100002602                                        0.74652   \n",
       "780100011404                                        0.74652   \n",
       "780100017405                                        0.57727   \n",
       "780100029602                                        0.74652   \n",
       "\n",
       "              CLEANSED_ZIPCODE__*__voluntary_discos__*__sum  \n",
       "chc_id                                                       \n",
       "780100001104                                          193.0  \n",
       "780100002602                                          193.0  \n",
       "780100011404                                          193.0  \n",
       "780100017405                                           39.0  \n",
       "780100029602                                          193.0  \n",
       "\n",
       "[5 rows x 11396 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X_FE.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Remove Highly Correlated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use Feature Engineered Data as new TRAIN/TEST Datasets\n",
    "# train_X = df_train_FE_NUM.merge(train_X_FE, how='inner', left_index=True, right_index=True).drop('status', axis=1)\n",
    "# test_X  = df_test_FE_NUM.merge(test_X_FE, how='inner', left_index=True, right_index=True).drop('status', axis=1)\n",
    "# print(train_X.shape)\n",
    "# print(train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # (1) Instantiate\n",
    "# # default correlation_threshold=0.90    \n",
    "# PP_Correlated = FE.Remove_CorrelatedFeatures(correlation_threshold=0.90)\n",
    "\n",
    "\n",
    "# # (2) fit()\n",
    "# # default: y=None\n",
    "# # Note: data = train_X_Missing\n",
    "# PP_Correlated.fit(train_X, train_y)\n",
    "\n",
    "# # list\n",
    "# print('\\n' + '*'*50 + '\\nFeatures Dropped Due to Multicollinearity\\n' + '*'*50 + '\\n', \n",
    "#       PP_Correlated.features_dropped_)\n",
    "\n",
    "# # dataframe\n",
    "# summary_dropped_Correlated     = PP_Correlated.summary_dropped_\n",
    "\n",
    "\n",
    "# # (3) transform()\n",
    "# train_X_Correlated = PP_Correlated.transform(train_X)\n",
    "# test_X_Correlated  = PP_Correlated.transform(test_X)\n",
    "\n",
    "# # Feature Dimension\n",
    "# print('\\n' + '*'*50 + '\\nBefore vs After Transformation\\n' + '*'*50)\n",
    "# print('TRAIN: Before Transformation:' + str(train_X.shape))\n",
    "# print('TRAIN: After Transformation: ' + str(train_X_Correlated.shape))\n",
    "# print('TEST:  After Transformation: ' + str(test_X_Correlated.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary_dropped_Correlated.head(n=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
